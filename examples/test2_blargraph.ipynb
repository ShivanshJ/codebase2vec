{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.text_splitter import CodeSplitter\n",
    "from llama_index.packs.code_hierarchy import CodeHierarchyNodeParser\n",
    "\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def print_python(python_text):\n",
    "    \"\"\"This function prints python text in ipynb nicely formatted.\"\"\"\n",
    "    display(Markdown(\"```python\\n\" + python_text + \"```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalGraphInfo:\n",
    "    visited_nodes: dict\n",
    "    imports: dict\n",
    "    import_aliases: dict\n",
    "    autoloaded_modules: dict\n",
    "    inheritances: dict\n",
    "    entity_id: str\n",
    "    aliases: dict\n",
    "\n",
    "    def __init__(self, entity_id: str):\n",
    "        self.visited_nodes = {}\n",
    "        self.imports = {}\n",
    "        self.import_aliases = {}\n",
    "        self.auto_loaded_imports = {}\n",
    "        self.inheritances = {}\n",
    "        self.entity_id = entity_id\n",
    "        self.aliases = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "def _skip_file(path: Path) -> bool:\n",
    "    # skip lock files\n",
    "    path = path.name\n",
    "    if path.endswith(\"lock\") or path == \"package-lock.json\" or path == \"yarn.lock\":\n",
    "        return True\n",
    "    # skip tests and legacy directories\n",
    "    if path in [\"legacy\", \"test\"] and self.skip_tests:\n",
    "        return True\n",
    "    # skip hidden files\n",
    "    if path.startswith(\".\"):\n",
    "        return True\n",
    "    # skip images\n",
    "    if path.endswith(\".png\") or path.endswith(\".jpg\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _remove_non_ascii(text):\n",
    "    # Define the regular expression pattern to match ascii characters\n",
    "    pattern = re.compile(r\"[^\\x00-\\x7F]+\")\n",
    "    # Replace ascii characters with an empty string\n",
    "    cleaned_text = pattern.sub(\"\", text)\n",
    "    return cleaned_text\n",
    "\n",
    "def _skip_directory(directory: Path) -> bool:\n",
    "    # skip hidden directories\n",
    "    if directory.name.startswith(\".\"):\n",
    "        return True\n",
    "    return directory == \"__pycache__\" or directory == \"node_modules\"\n",
    "\n",
    "\n",
    "\n",
    "## --- BASE PARSER\n",
    "def generate_node_id(path: str, company_id: str):\n",
    "    # Concatenate path and signature\n",
    "    combined_string = f\"{company_id}:{path}\"\n",
    "    hash_object = hashlib.md5()\n",
    "    hash_object.update(combined_string.encode(\"utf-8\"))\n",
    "    # Get the hexadecimal representation of the hash\n",
    "    node_id = hash_object.hexdigest()\n",
    "    return node_id\n",
    "\n",
    "def is_package(path: str) -> bool:\n",
    "    return os.path.exists(os.path.join(path, \"__init__.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format node functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "from llama_index.core.schema import BaseNode\n",
    "\n",
    "# ---- Schema\n",
    "\n",
    "class MyNodeType(Enum):\n",
    "    CODE_BLOCK = \"CODE_BLOCK\"\n",
    "    FUNCTION = \"FUNCTION\"\n",
    "    CLASS = \"CLASS\"\n",
    "    FILE = \"FILE\"\n",
    "    PACKAGE = \"PACKAGE\"\n",
    "    FOLDER = \"FOLDER\"\n",
    "\n",
    "\n",
    "# ---- Attributes\n",
    "@dataclass\n",
    "class BaseAttributes:\n",
    "    name: str\n",
    "    text: Optional[str] = None\n",
    "    function_calls: Optional[List[str]] = None\n",
    "    file_node_id: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class CodeBlockAttributes(BaseAttributes):\n",
    "    signature: str = None\n",
    "\n",
    "@dataclass\n",
    "class FunctionAttributes(BaseAttributes):\n",
    "    signature: str = None\n",
    "\n",
    "@dataclass\n",
    "class ClassAttributes(BaseAttributes):\n",
    "    signature: str = None\n",
    "    inheritances: List[str] = None\n",
    "\n",
    "@dataclass\n",
    "class FileAttributes(BaseAttributes):\n",
    "    path: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class DirectoryAttributes(BaseAttributes):\n",
    "    path: str = None\n",
    "    level: int = None\n",
    "\n",
    "\n",
    "# ---- Node\n",
    "@dataclass\n",
    "class MyNode:\n",
    "    type: MyNodeType\n",
    "    attributes: BaseAttributes\n",
    "\n",
    "@dataclass\n",
    "class Relationship:\n",
    "    source_id: str\n",
    "    target_id: str\n",
    "    type: str\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class format_nodes:\n",
    "\n",
    "    @staticmethod\n",
    "    def format_code_block_node( node_text: str, scope: dict, function_calls: List[str], file_node_id: str ) -> MyNode:\n",
    "        attributes = CodeBlockAttributes(\n",
    "            name=scope[\"name\"],\n",
    "            signature=scope[\"signature\"],\n",
    "            text=node_text,\n",
    "            function_calls=function_calls,\n",
    "            file_node_id=file_node_id\n",
    "        )\n",
    "        return MyNode(type=NodeType.CODE_BLOCK, attributes=attributes)\n",
    "\n",
    "    @staticmethod\n",
    "    def format_function_node( node_text: str, scope: dict, function_calls: List[str], file_node_id: str ) -> MyNode:\n",
    "        attributes = FunctionAttributes(\n",
    "            name=scope[\"name\"],\n",
    "            signature=scope[\"signature\"],\n",
    "            text=node_text,\n",
    "            function_calls=function_calls,\n",
    "            file_node_id=file_node_id\n",
    "        )\n",
    "        return MyNode(type=NodeType.FUNCTION, attributes=attributes)\n",
    "\n",
    "    @staticmethod\n",
    "    def format_class_node( node_text: str, scope: dict, file_node_id: str, inheritances: List[str], function_calls: List[str] ) -> MyNode:\n",
    "        attributes = ClassAttributes(\n",
    "            name=scope[\"name\"],\n",
    "            signature=scope[\"signature\"],\n",
    "            text=node_text,\n",
    "            file_node_id=file_node_id,\n",
    "            inheritances=inheritances,\n",
    "            function_calls=function_calls\n",
    "        )\n",
    "        return MyNode(type=NodeType.CLASS, attributes=attributes)\n",
    "\n",
    "    @staticmethod\n",
    "    def format_file_node(node_text: str, no_extension_path: str, function_calls: List[str] = None, file_node_id = None ) -> MyNode:\n",
    "        attributes = FileAttributes(\n",
    "            name=os.path.basename(no_extension_path),\n",
    "            path=no_extension_path,\n",
    "            text=node_text,\n",
    "            file_node_id=file_node_id,\n",
    "            function_calls=function_calls\n",
    "        )\n",
    "        return MyNode(type=NodeType.FILE, attributes=attributes)\n",
    "\n",
    "    @staticmethod\n",
    "    def format_directory_node( path: str, package: bool, level: int ) -> MyNode:\n",
    "        attributes = DirectoryAttributes(\n",
    "            name=os.path.basename(path),\n",
    "            path=f\"{path}/\",\n",
    "            level=level\n",
    "        )\n",
    "        return MyNode(\n",
    "            type=NodeType.PACKAGE if package else NodeType.FOLDER,\n",
    "            attributes=attributes\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse file (BaseParser)\n",
    "\n",
    "1. responsible for getting parser\n",
    "2. calling SimpleDirectory & CodeHierarchyNodeParser from llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "class BaseParser:\n",
    "    def __init__(\n",
    "        self,\n",
    "        language: str,\n",
    "        wildcard: str,\n",
    "        extension: str,\n",
    "        import_path_separator: str = \".\",\n",
    "        global_graph_info: GlobalGraphInfo = {\"alias\": {}},\n",
    "    ):\n",
    "        self.language = language\n",
    "        self.wildcard = wildcard\n",
    "        self.extension = extension\n",
    "        self.import_path_separator = import_path_separator\n",
    "        self.global_graph_info = global_graph_info\n",
    "\n",
    "\n",
    "    def parse(self, file_path: str, root_path: str, global_graph_info: GlobalGraphInfo, level: int):\n",
    "        path = Path(file_path)\n",
    "        if not path.exists():\n",
    "            print(f\"File {file_path} does not exist.\")\n",
    "            raise FileNotFoundError\n",
    "\n",
    "        documents = SimpleDirectoryReader(\n",
    "            input_files=[path],\n",
    "            file_metadata=lambda x: {\"filepath\": x},\n",
    "        ).load_data()\n",
    "        # Bug related to llama-index it's safer to remove non-ascii characters. Could be removed in the future\n",
    "        documents[0].text = _remove_non_ascii(documents[0].text)\n",
    "        code = CodeHierarchyNodeParser(\n",
    "            language=self.language,\n",
    "            chunk_min_characters=3,\n",
    "            signature_identifiers=self.signature_identifiers,\n",
    "        )\n",
    "        try:\n",
    "            split_nodes = code.get_nodes_from_documents(documents)  # Each node has a \"text\" attribute which is the code block, & a \"node id\"\n",
    "        except TimeoutError:\n",
    "            print(f\"Timeout error: {file_path}\")\n",
    "            return [], [], {}\n",
    "\n",
    "        node_list = []\n",
    "        edges_list = []\n",
    "        assignment_dict = {}\n",
    "        # -- Make file node & relation\n",
    "        file_node, file_relations = self.__process_node__(\n",
    "            split_nodes.pop(0), file_path, \"\", global_graph_info, assignment_dict, documents[0], level  # split_nodes.pop(0) gives code structure of the full file\n",
    "        )\n",
    "        node_list.append(file_node)\n",
    "        edges_list.extend(file_relations)\n",
    "        # -- Make all other nodes within file\n",
    "        for node in split_nodes:\n",
    "            processed_node, relationships = self.__process_node__(\n",
    "                node,\n",
    "                file_path,\n",
    "                file_node[\"attributes\"][\"node_id\"],\n",
    "                global_graph_info,\n",
    "                assignment_dict,\n",
    "                documents[0],\n",
    "                level,\n",
    "            )\n",
    "            node_list.append(processed_node)\n",
    "            edges_list.extend(relationships)\n",
    "\n",
    "        post_processed_node_list = []\n",
    "        for node in node_list:\n",
    "            node = self._post_process_node(node, global_graph_info)\n",
    "            post_processed_node_list.append(node)\n",
    "\n",
    "        imports = self._get_imports(str(path), node_list[0][\"attributes\"][\"node_id\"], root_path)\n",
    "\n",
    "        return post_processed_node_list, edges_list, imports\n",
    "\n",
    "\n",
    "\n",
    "    def __process_node__(\n",
    "        self,\n",
    "        node: BaseNode,\n",
    "        file_path: str,\n",
    "        file_node_id: str,\n",
    "        global_graph_info: GlobalGraphInfo,\n",
    "        assignment_dict: dict,\n",
    "        document: Document,\n",
    "        level: int,\n",
    "    ):\n",
    "        relationships = []\n",
    "        inclusive_scopes = node.metadata[\"inclusive_scopes\"]\n",
    "        scope = inclusive_scopes[-1] if inclusive_scopes else None\n",
    "        type_node = scope[\"type\"] if scope else \"file\"\n",
    "        parent_level = level\n",
    "        leaf = False\n",
    "\n",
    "        function_calls = self._get_function_calls(node, assignment_dict)\n",
    "        if type_node in self.scopes_names[\"function\"]:\n",
    "            core_node = format_nodes.format_function_node(node, scope, function_calls, file_node_id)\n",
    "        elif type_node in self.scopes_names[\"class\"]:\n",
    "            inheritances = self._get_inheritances(node)\n",
    "            core_node = format_nodes.format_class_node(node, scope, file_node_id, inheritances, function_calls)\n",
    "        else:\n",
    "            core_node = format_nodes.format_file_node(node, file_path, function_calls)\n",
    "\n",
    "        parent_level = self._get_parent_level(node, global_graph_info, level)\n",
    "\n",
    "        node_path = self.get_node_path(node)\n",
    "        parent_path = \".\".join(node_path.split(\".\")[:-1])\n",
    "\n",
    "        parent_id = generate_node_id(parent_path, global_graph_info.entity_id)\n",
    "        node_id = generate_node_id(node_path, global_graph_info.entity_id)\n",
    "        if type_node in self.scopes_names[\"class\"]:\n",
    "            global_graph_info.inheritances[node_id] = inheritances\n",
    "\n",
    "        relation_type = scope[\"type\"] if scope else \"\"\n",
    "        if self.relation_types_map.get(relation_type) is not None:\n",
    "            relationships.append(\n",
    "                {\n",
    "                    \"sourceId\": parent_id,\n",
    "                    \"targetId\": node_id,\n",
    "                    \"type\": self.relation_types_map.get(relation_type, \"UNKNOWN\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        start_line, end_line = self._get_lines_range(\n",
    "            document.text, node.metadata[\"start_byte\"], node.metadata[\"end_byte\"]\n",
    "        )\n",
    "\n",
    "        horizontal_attributes = {\n",
    "            \"start_line\": start_line,\n",
    "            \"end_line\": end_line,\n",
    "            \"path\": node_path,\n",
    "            \"file_path\": file_path,\n",
    "            \"level\": parent_level + 1,\n",
    "            \"leaf\": leaf,\n",
    "            \"node_id\": node_id,\n",
    "        }\n",
    "\n",
    "        processed_node = {\n",
    "            **core_node,\n",
    "            \"attributes\": {\n",
    "                **core_node[\"attributes\"],\n",
    "                **horizontal_attributes,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        global_graph_info.imports[node_path] = {\n",
    "            \"id\": processed_node[\"attributes\"][\"node_id\"],\n",
    "            \"type\": processed_node[\"type\"],\n",
    "            \"node\": processed_node,\n",
    "        }\n",
    "\n",
    "        global_graph_info.visited_nodes[node.node_id] = {\"level\": parent_level + 1, \"generated_id\": node_id}\n",
    "        return processed_node, relationships\n",
    "\n",
    "\n",
    "    # Takes out the node_id from llama_index's CodeNodeHierarchyParser\n",
    "    def _post_process_node(self, node: dict, global_graph_info: GlobalGraphInfo):\n",
    "        text = node[\"attributes\"][\"text\"]\n",
    "        # Extract the node_id using re.search\n",
    "        matches = re.findall(r\"Code replaced for brevity\\. See node_id ([0-9a-fA-F-]+)\", text)\n",
    "        for match in matches:\n",
    "            extracted_node_id = match\n",
    "            # Get the mapped_generated_id using the extracted node_id\n",
    "            mapped_generated_id = global_graph_info.visited_nodes.get(extracted_node_id, {}).get(\"generated_id\")\n",
    "            if mapped_generated_id is not None:\n",
    "                # Replace the extracted node_id with the mapped_generated_id\n",
    "                updated_text = re.sub(\n",
    "                    rf\"Code replaced for brevity\\. See node_id {extracted_node_id}\",\n",
    "                    f\"Code replaced for brevity. See node_id {mapped_generated_id}\",\n",
    "                    text,\n",
    "                )\n",
    "                text = updated_text\n",
    "        node[\"attributes\"][\"text\"] = text\n",
    "        return node\n",
    "\n",
    "\n",
    "    def _get_lines_range(self, file_contents, start_byte, end_byte):\n",
    "        start_line = file_contents.count(\"\\n\", 0, start_byte) + 1\n",
    "        end_line = file_contents.count(\"\\n\", 0, end_byte) + 1\n",
    "\n",
    "        return (start_line, end_line)\n",
    "\n",
    "\n",
    "\n",
    "    def get_node_path(self, node: BaseNode):\n",
    "        file_path = node.metadata[\"filepath\"]\n",
    "        scopes = node.metadata[\"inclusive_scopes\"]\n",
    "        scopes_path = reduce(lambda x, y: x + \".\" + y[\"name\"], scopes, \"\")\n",
    "        no_extension_path = self.remove_extensions(file_path)\n",
    "        node_path = no_extension_path.replace(\"/\", \".\")\n",
    "\n",
    "        if len(scopes_path) > 0:\n",
    "            return node_path + scopes_path\n",
    "        return node_path\n",
    "\n",
    "    def _get_parent_level(self, node: BaseNode, global_graph_info: GlobalGraphInfo, level: int):\n",
    "        parent_level = level\n",
    "        try:\n",
    "            parent = node.parent_node\n",
    "        except Exception:\n",
    "            parent = None\n",
    "        if parent:\n",
    "            parent_level = global_graph_info.visited_nodes.get(parent.node_id, {}).get(\"level\", level)\n",
    "        return parent_level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parsers:\n",
    "    def __init__(self, global_graph_info: GlobalGraphInfo, root_path: str):\n",
    "        self.python = PythonParser(global_graph_info)\n",
    "        pass\n",
    "\n",
    "    def get_parser(self, path: str):\n",
    "        extension = path[path.rfind(\".\") :]\n",
    "        if extension == \".py\":\n",
    "            return self.python\n",
    "        return None\n",
    "\n",
    "class PythonParser(BaseParser):\n",
    "    def __init__(self, global_graph_info: GlobalGraphInfo):\n",
    "        super().__init__(\"python\", \"*\", \".py\", \".\", global_graph_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from typing import Optional, Set, Tuple, List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "class GraphConstructor:\n",
    "    global_graph_info: GlobalGraphInfo\n",
    "    root: str\n",
    "    skip_tests: bool\n",
    "    parsers: Parsers\n",
    "    max_workers: int = 50\n",
    "\n",
    "\n",
    "    def __init__(self, entity_id: str, root: str, max_workers: Optional[int] = None):\n",
    "        self.global_graph_info = GlobalGraphInfo(entity_id=entity_id)\n",
    "        self.parsers = Parsers(self.global_graph_info, root)\n",
    "        self.root = root\n",
    "        self.skip_tests = True\n",
    "        if max_workers is not None:\n",
    "            self.max_workers = max_workers\n",
    "            \n",
    "\n",
    "    def build_graph(self):\n",
    "        # process every node to create the graph structure\n",
    "        print(\"Building graph...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        nodes, relationships, imports = self.__scan_directory(self.root)\n",
    "\n",
    "        # relate imports between file nodes\n",
    "        relationships.extend(self._relate_imports(imports))\n",
    "        # relate functions calls\n",
    "        relationships.extend(self._relate_constructor_calls(nodes, imports))\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Execution time: {execution_time} seconds\")\n",
    "        return nodes, relationships\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "        helpers\n",
    "    \"\"\"\n",
    "\n",
    "    def __scan_directory(self, path: str, parent_id: Optional[str] = None, level: int = 0, visited: Optional[Set[str]] = None,\n",
    "    ) -> Tuple[List[Dict], List[Dict], Dict]:\n",
    "\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "\n",
    "        nodes, relationships, imports = [],[],[]\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Directory {path} not found\")\n",
    "        if path.endswith(\"tests\") or path.endswith(\"test\"):\n",
    "            return nodes, relationships, imports\n",
    "        if path in visited:\n",
    "            return nodes, relationships, imports\n",
    "\n",
    "        visited.add(path)\n",
    "        # Check if the directory is a package, logic for python\n",
    "        package = is_package(path)\n",
    "        core_directory_node = format_nodes.format_directory_node(path, package, level)\n",
    "        directory_node_id = generate_node_id(path, self.global_graph_info.entity_id)\n",
    "        directory_node = {\n",
    "            **core_directory_node,\n",
    "            \"attributes\": {**core_directory_node[\"attributes\"], \"node_id\": directory_node_id},\n",
    "        }\n",
    "        print (directory_node)\n",
    "        nodes.append(directory_node)\n",
    "        if parent_id is not None:\n",
    "            # relationship only exists when we are recursing (DFS)\n",
    "            relationships.append(\n",
    "                {\n",
    "                    \"sourceId\": parent_id,\n",
    "                    \"targetId\": directory_node_id,\n",
    "                    \"type\": \"CONTAINS\",\n",
    "                }\n",
    "            )\n",
    "        try:\n",
    "            entries = list(os.scandir(path))\n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied: {path}\")\n",
    "            return nodes, relationships, imports\n",
    "        # Submit all entries to the executor\n",
    "        with ThreadPoolExecutor(max_workers=min(self.max_workers, os.cpu_count() or 1)) as executor:\n",
    "            future_to_entry = {executor.submit(self.__process_entry, entry, directory_node_id): entry for entry in entries}\n",
    "            for future in as_completed(future_to_entry):\n",
    "                try:\n",
    "                    entry_nodes, entry_relationships, entry_imports, entry_visited = future.result()\n",
    "                    nodes.extend(entry_nodes)\n",
    "                    relationships.extend(entry_relationships)\n",
    "                    imports.update(entry_imports)\n",
    "                    visited.update(entry_visited)\n",
    "                except Exception as exc:\n",
    "                    entry = future_to_entry[future]\n",
    "                    print(f\"Generated an exception: {entry.path} -> {exc}\")\n",
    "                    traceback.print_exc()\n",
    "        return nodes, relationships, imports\n",
    "\n",
    "    \n",
    "    def __process_entry(entry: Path, directory_node_id: str) -> Tuple[List[Dict], List[Dict], Dict, Set[str]]:\n",
    "        local_nodes: List[Dict] = []\n",
    "        local_relationships: List[Dict] = []\n",
    "        local_imports: Dict = {}\n",
    "        local_visited: Set[str] = set()\n",
    "        if _skip_file(entry.name):\n",
    "            return local_nodes, local_relationships, local_imports, local_visited\n",
    "\n",
    "        if entry.is_file():\n",
    "            return self.__process_entry_file(entry)\n",
    "        elif entry.is_dir():\n",
    "            return self.__process_entry_dir(entry)\n",
    "\n",
    "        return local_nodes, local_relationships, local_imports, local_visited\n",
    "\n",
    "\n",
    "\n",
    "    def __process_entry_file(self, entry: Path, directory_node_id: str):\n",
    "        local_nodes: List[Dict] = []\n",
    "        local_relationships: List[Dict] = []\n",
    "        local_imports: Dict = {}\n",
    "        local_visited: Set[str] = set()\n",
    "\n",
    "        parser = self.parsers.get_parser(entry.name)\n",
    "        if parser:\n",
    "            entry_name = entry.name.split(parser.extension)[0]\n",
    "            try:\n",
    "                processed_nodes, relations, file_imports = parser.parse_file(entry.path, self.root, global_graph_info=self.global_graph_info, level=level)\n",
    "            except Exception:\n",
    "                print(f\"Error parsing file {entry.path}\")\n",
    "                print(traceback.format_exc())\n",
    "                return local_nodes, local_relationships, local_imports, local_visited\n",
    "            if processed_nodes:\n",
    "                file_root_node_id = processed_nodes[0][\"attributes\"][\"node_id\"]\n",
    "                local_nodes.extend(processed_nodes)\n",
    "                local_relationships.extend(relations)\n",
    "                local_relationships.append(\n",
    "                    {\n",
    "                        \"sourceId\": directory_node_id,\n",
    "                        \"targetId\": file_root_node_id,\n",
    "                        \"type\": \"CONTAINS\",\n",
    "                    }\n",
    "                )\n",
    "                local_imports.update(file_imports)\n",
    "\n",
    "                global_import_key = (directory_path + entry_name).replace(\"/\", \".\")\n",
    "                self.global_graph_info.imports[global_import_key] = {\n",
    "                    \"id\": file_root_node_id,\n",
    "                    \"type\": \"FILE\",\n",
    "                    \"node\": processed_nodes[0],\n",
    "                }\n",
    "            else:\n",
    "                self.global_graph_info.import_aliases.update(file_imports)\n",
    "        else:\n",
    "            # else make a file node\n",
    "            try:\n",
    "                with open(entry.path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    text = file.read()\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Error reading file {entry.path}\")\n",
    "                return local_nodes, local_relationships, local_imports, local_visited\n",
    "\n",
    "            path = str(entry.path).replace(\"/\", \".\")\n",
    "            file_node = format_nodes.format_file_node(text, path, generate_node_id(path, self.global_graph_info.entity_id)){\n",
    "                \"type\": \"FILE\",\n",
    "                \"attributes\": {\n",
    "                    \"node_id\": generate_node_id(path, self.global_graph_info.entity_id),\n",
    "                    \"text\": text,\n",
    "                },\n",
    "            }\n",
    "            local_nodes.append(file_node)\n",
    "            local_relationships.append(\n",
    "                {\n",
    "                    \"sourceId\": directory_node_id,\n",
    "                    \"targetId\": file_node[\"attributes\"][\"node_id\"],\n",
    "                    \"type\": \"CONTAINS\",\n",
    "                }\n",
    "            )\n",
    "        return local_nodes, local_relationships, local_imports, local_visited\n",
    "\n",
    "\n",
    "\n",
    "    def __process_entry_dir(self, entry: Path) -> Tuple[List[Dict], List[Dict], Dict, Set[str]]:\n",
    "        local_nodes: List[Dict] = []\n",
    "        local_relationships: List[Dict] = []\n",
    "        local_imports: Dict = {}\n",
    "        local_visited: Set[str] = set()\n",
    "\n",
    "        if _skip_directory(entry.name):\n",
    "            return local_nodes, local_relationships, local_imports, local_visited\n",
    "\n",
    "        sub_nodes, sub_relationships, sub_imports = self.__scan_directory(\n",
    "            entry.path, directory_node_id, level + 1, visited\n",
    "        )\n",
    "        local_nodes.extend(sub_nodes)\n",
    "        local_relationships.extend(sub_relationships)\n",
    "        local_imports.update(sub_imports)\n",
    "        return local_nodes, local_relationships, local_imports, local_visited\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
