{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.text_splitter import CodeSplitter\n",
    "from llama_index.packs.code_hierarchy import CodeHierarchyNodeParser\n",
    "\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def print_python(python_text):\n",
    "    \"\"\"This function prints python text in ipynb nicely formatted.\"\"\"\n",
    "    display(Markdown(\"```python\\n\" + python_text + \"```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "def _skip_file(path: Path) -> bool:\n",
    "    # skip lock files\n",
    "    path = path.name\n",
    "    if path.endswith(\"lock\") or path == \"package-lock.json\" or path == \"yarn.lock\":\n",
    "        return True\n",
    "    # skip tests and legacy directories\n",
    "    if path in [\"legacy\", \"test\"] and self.skip_tests:\n",
    "        return True\n",
    "    # skip hidden files\n",
    "    if path.startswith(\".\"):\n",
    "        return True\n",
    "    # skip images\n",
    "    if path.endswith(\".png\") or path.endswith(\".jpg\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _remove_non_ascii(text):\n",
    "    # Define the regular expression pattern to match ascii characters\n",
    "    pattern = re.compile(r\"[^\\x00-\\x7F]+\")\n",
    "    # Replace ascii characters with an empty string\n",
    "    cleaned_text = pattern.sub(\"\", text)\n",
    "    return cleaned_text\n",
    "\n",
    "def _skip_directory(directory: Path) -> bool:\n",
    "    # skip hidden directories\n",
    "    if directory.name.startswith(\".\"):\n",
    "        return True\n",
    "    return directory == \"__pycache__\" or directory == \"node_modules\"\n",
    "\n",
    "\n",
    "\n",
    "## --- BASE PARSER\n",
    "def generate_node_id(path: str, company_id: str):\n",
    "    # Concatenate path and signature\n",
    "    combined_string = f\"{company_id}:{path}\"\n",
    "    hash_object = hashlib.md5()\n",
    "    hash_object.update(combined_string.encode(\"utf-8\"))\n",
    "    # Get the hexadecimal representation of the hash\n",
    "    node_id = hash_object.hexdigest()\n",
    "    return node_id\n",
    "\n",
    "def is_package(path: str) -> bool:\n",
    "    return os.path.exists(os.path.join(path, \"__init__.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format node functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_index.core.schema import BaseNode\n",
    "\n",
    "\n",
    "\n",
    "class format_nodes:\n",
    "\n",
    "    @staticmethod\n",
    "    def format_plain_code_block_node(node: BaseNode, scope: dict, function_calls: list[str], file_node_id: str) -> dict:\n",
    "        name = scope[\"name\"]\n",
    "        signature = scope[\"signature\"]\n",
    "\n",
    "        processed_node = {\n",
    "            \"type\": \"CODE_BLOCK\",\n",
    "            \"attributes\": {\n",
    "                \"name\": name,\n",
    "                \"signature\": signature,\n",
    "                \"text\": node.text,\n",
    "                \"function_calls\": function_calls,\n",
    "                \"file_node_id\": file_node_id,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        return processed_node\n",
    "\n",
    "    @staticmethod\n",
    "    def format_function_node(node: BaseNode, scope: dict, function_calls: list[str], file_node_id: str) -> dict:\n",
    "        name = scope[\"name\"]\n",
    "        signature = scope[\"signature\"]\n",
    "\n",
    "        processed_node = {\n",
    "            \"type\": \"FUNCTION\",\n",
    "            \"attributes\": {\n",
    "                \"name\": name,\n",
    "                \"signature\": signature,\n",
    "                \"text\": node.text,\n",
    "                \"function_calls\": function_calls,\n",
    "                \"file_node_id\": file_node_id,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        return processed_node\n",
    "\n",
    "    @staticmethod\n",
    "    def format_class_node( node: BaseNode, scope: dict, file_node_id: str, inheritances: list[str], function_calls: list[str] ) -> dict:\n",
    "        name = scope[\"name\"]\n",
    "        signature = scope[\"signature\"]\n",
    "\n",
    "        processed_node = {\n",
    "            \"type\": \"CLASS\",\n",
    "            \"attributes\": {\n",
    "                \"name\": name,\n",
    "                \"signature\": signature,\n",
    "                \"text\": node.text,\n",
    "                \"file_node_id\": file_node_id,\n",
    "                \"inheritances\": inheritances,\n",
    "                \"function_calls\": function_calls,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        return processed_node\n",
    "\n",
    "    @staticmethod\n",
    "    def format_file_node(node: BaseNode, no_extension_path: str, function_calls: list[str]) -> dict:\n",
    "        processed_node = {\n",
    "            \"type\": \"FILE\",\n",
    "            \"attributes\": {\n",
    "                \"text\": node.text,\n",
    "                \"function_calls\": function_calls,\n",
    "                \"name\": os.path.basename(no_extension_path),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        return processed_node\n",
    "\n",
    "    @staticmethod\n",
    "    def format_directory_node(path: str, package: bool, level: int) -> dict:\n",
    "        processed_node = {\n",
    "            \"attributes\": {\n",
    "                \"path\": path + \"/\",\n",
    "                \"name\": os.path.basename(path),\n",
    "                \"level\": level,\n",
    "            },\n",
    "            \"type\": \"PACKAGE\" if package else \"FOLDER\",\n",
    "        }\n",
    "\n",
    "        return processed_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (4139504019.py, line 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 49\u001b[0;36m\u001b[0m\n\u001b[0;31m    visited = set()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from core.utils import GlobalGraphInfo\n",
    "\n",
    "\n",
    "class GraphConstructor:\n",
    "    global_graph_info: GlobalGraphInfo\n",
    "    root: str\n",
    "    skip_tests: bool\n",
    "    parsers: Parsers\n",
    "    max_workers: int = 50\n",
    "\n",
    "\n",
    "    def __init__(self, entity_id: str, root: str, max_workers: Optional[int] = None):\n",
    "        self.global_graph_info = GlobalGraphInfo(entity_id=entity_id)\n",
    "        self.parsers = Parsers(self.global_graph_info, root)\n",
    "        self.root = root\n",
    "        self.skip_tests = True\n",
    "        if max_workers is not None:\n",
    "            self.max_workers = max_workers\n",
    "            \n",
    "\n",
    "    def build_graph(self):\n",
    "        # process every node to create the graph structure\n",
    "        print(\"Building graph...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        nodes, relationships, imports = self._scan_directory(self.root)\n",
    "\n",
    "        # relate imports between file nodes\n",
    "        relationships.extend(self._relate_imports(imports))\n",
    "        # relate functions calls\n",
    "        relationships.extend(self._relate_constructor_calls(nodes, imports))\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Execution time: {execution_time} seconds\")\n",
    "        return nodes, relationships\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "        helpers\n",
    "    \"\"\"\n",
    "    def _scan_directory(self, path: str, parent_id: Optional[str] = None, level: int = 0, visited: Optional[Set[str]] = None,\n",
    "    ) -> Tuple[List[Dict], List[Dict], Dict]:\n",
    "\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "\n",
    "        nodes: List[Dict] = []\n",
    "        relationships: List[Dict] = []\n",
    "        imports: Dict = {}\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Directory {path} not found\")\n",
    "        if path.endswith(\"tests\") or path.endswith(\"test\"):\n",
    "            return nodes, relationships, imports\n",
    "\n",
    "        if path in visited:\n",
    "            return nodes, relationships, imports\n",
    "        visited.add(path)\n",
    "\n",
    "        # Check if the directory is a package, logic for python\n",
    "        package = is_package(path)\n",
    "\n",
    "        core_directory_node = format_nodes.format_directory_node(path, package, level)\n",
    "        directory_node_id = generate_node_id(path, self.global_graph_info.entity_id)\n",
    "        directory_node = {\n",
    "            **core_directory_node,\n",
    "            \"attributes\": {**core_directory_node[\"attributes\"], \"node_id\": directory_node_id},\n",
    "        }\n",
    "        print (directory_node)\n",
    "        nodes.append(directory_node)\n",
    "\n",
    "        if parent_id is not None:\n",
    "            # relationship only exists when we are recursing (DFS)\n",
    "            relationships.append(\n",
    "                {\n",
    "                    \"sourceId\": parent_id,\n",
    "                    \"targetId\": directory_node_id,\n",
    "                    \"type\": \"CONTAINS\",\n",
    "                }\n",
    "            )\n",
    "        try:\n",
    "            entries = list(os.scandir(path))\n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied: {path}\")\n",
    "            return nodes, relationships, imports\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=min(self.max_workers, os.cpu_count() or 1)) as executor:\n",
    "            # Submit all entries to the executor\n",
    "            future_to_entry = {executor.submit(process_entry, entry): entry for entry in entries}\n",
    "            for future in as_completed(future_to_entry):\n",
    "                try:\n",
    "                    entry_nodes, entry_relationships, entry_imports, entry_visited = future.result()\n",
    "                    nodes.extend(entry_nodes)\n",
    "                    relationships.extend(entry_relationships)\n",
    "                    imports.update(entry_imports)\n",
    "                    visited.update(entry_visited)\n",
    "                except Exception as exc:\n",
    "                    entry = future_to_entry[future]\n",
    "                    print(f\"Generated an exception: {entry.path} -> {exc}\")\n",
    "                    traceback.print_exc()\n",
    "\n",
    "        return nodes, relationships, imports\n",
    "\n",
    "    \n",
    "    def process_entry(entry) -> Tuple[List[Dict], List[Dict], Dict, Set[str]]:\n",
    "        local_nodes: List[Dict] = []\n",
    "        local_relationships: List[Dict] = []\n",
    "        local_imports: Dict = {}\n",
    "        local_visited: Set[str] = set()\n",
    "\n",
    "        if self._skip_file(entry.name):\n",
    "            return local_nodes, local_relationships, local_imports, local_visited\n",
    "\n",
    "        if entry.is_file():\n",
    "            parser: BaseParser | None = self.parsers.get_parser(entry.name)\n",
    "            # If the file is a supported language, parse it\n",
    "            if parser:\n",
    "                entry_name = entry.name.split(parser.extension)[0]\n",
    "                try:\n",
    "                    processed_nodes, relations, file_imports = parser.parse_file(\n",
    "                        entry.path,\n",
    "                        self.root,\n",
    "                        global_graph_info=self.global_graph_info,\n",
    "                        level=level,\n",
    "                    )\n",
    "                except Exception:\n",
    "                    print(f\"Error parsing file {entry.path}\")\n",
    "                    print(traceback.format_exc())\n",
    "                    return local_nodes, local_relationships, local_imports, local_visited\n",
    "\n",
    "                if processed_nodes:\n",
    "                    file_root_node_id = processed_nodes[0][\"attributes\"][\"node_id\"]\n",
    "                    local_nodes.extend(processed_nodes)\n",
    "                    local_relationships.extend(relations)\n",
    "                    local_relationships.append(\n",
    "                        {\n",
    "                            \"sourceId\": directory_node_id,\n",
    "                            \"targetId\": file_root_node_id,\n",
    "                            \"type\": \"CONTAINS\",\n",
    "                        }\n",
    "                    )\n",
    "                    local_imports.update(file_imports)\n",
    "\n",
    "                    global_import_key = (directory_path + entry_name).replace(\"/\", \".\")\n",
    "                    self.global_graph_info.imports[global_import_key] = {\n",
    "                        \"id\": file_root_node_id,\n",
    "                        \"type\": \"FILE\",\n",
    "                        \"node\": processed_nodes[0],\n",
    "                    }\n",
    "                else:\n",
    "                    self.global_graph_info.import_aliases.update(file_imports)\n",
    "            else:\n",
    "                try:\n",
    "                    with open(entry.path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        text = file.read()\n",
    "                except UnicodeDecodeError:\n",
    "                    print(f\"Error reading file {entry.path}\")\n",
    "                    return local_nodes, local_relationships, local_imports, local_visited\n",
    "\n",
    "                path = str(entry.path).replace(\"/\", \".\")\n",
    "                file_node = {\n",
    "                    \"type\": \"FILE\",\n",
    "                    \"attributes\": {\n",
    "                        \"path\": path,\n",
    "                        \"file_path\": path,\n",
    "                        \"name\": entry.name,\n",
    "                        \"node_id\": BaseParser.generate_node_id(path, self.global_graph_info.entity_id),\n",
    "                        \"text\": text,\n",
    "                    },\n",
    "                }\n",
    "                local_nodes.append(file_node)\n",
    "                local_relationships.append(\n",
    "                    {\n",
    "                        \"sourceId\": directory_node_id,\n",
    "                        \"targetId\": file_node[\"attributes\"][\"node_id\"],\n",
    "                        \"type\": \"CONTAINS\",\n",
    "                    }\n",
    "                )\n",
    "        elif entry.is_dir():\n",
    "            if self._skip_directory(entry.name):\n",
    "                return local_nodes, local_relationships, local_imports, local_visited\n",
    "\n",
    "            sub_nodes, sub_relationships, sub_imports = self._scan_directory(\n",
    "                entry.path, directory_node_id, level + 1, visited\n",
    "            )\n",
    "            local_nodes.extend(sub_nodes)\n",
    "            local_relationships.extend(sub_relationships)\n",
    "            local_imports.update(sub_imports)\n",
    "\n",
    "        return local_nodes, local_relationships, local_imports, local_visited"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
