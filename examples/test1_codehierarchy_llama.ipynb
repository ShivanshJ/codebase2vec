{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.text_splitter import CodeSplitter\n",
    "from llama_index.packs.code_hierarchy import CodeHierarchyNodeParser\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def print_python(python_text):\n",
    "    \"\"\"This function prints python text in ipynb nicely formatted.\"\"\"\n",
    "    display(Markdown(\"```python\\n\" + python_text + \"```\"))\n",
    "\n",
    "\n",
    "def print_tail(text, n=1000):\n",
    "    \"\"\"This function prints the last n lines of text in ipynb nicely formatted.\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    last_n = lines[-n:]\n",
    "    display(Markdown(\"```\\n\" + \"\\n\".join(last_n) + \"\\n```\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_skip_file\u001b[39m(path: \u001b[43mPath\u001b[49m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# skip lock files\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlock\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m path \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage-lock.json\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m path \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myarn.lock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def _skip_file(path: Path) -> bool:\n",
    "    # skip lock files\n",
    "    path = path.name\n",
    "    if path.endswith(\"lock\") or path == \"package-lock.json\" or path == \"yarn.lock\":\n",
    "        return True\n",
    "    # skip tests and legacy directories\n",
    "    if path in [\"legacy\", \"test\"] and self.skip_tests:\n",
    "        return True\n",
    "    # skip hidden files\n",
    "    if path.startswith(\".\"):\n",
    "        return True\n",
    "    # skip images\n",
    "    if path.endswith(\".png\") or path.endswith(\".jpg\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _remove_non_ascii(text):\n",
    "    # Define the regular expression pattern to match ascii characters\n",
    "    pattern = re.compile(r\"[^\\x00-\\x7F]+\")\n",
    "    # Replace ascii characters with an empty string\n",
    "    cleaned_text = pattern.sub(\"\", text)\n",
    "    return cleaned_text\n",
    "\n",
    "def _skip_directory(directory: Path) -> bool:\n",
    "    # skip hidden directories\n",
    "    if directory.name.startswith(\".\"):\n",
    "        return True\n",
    "    return directory == \"__pycache__\" or directory == \"node_modules\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory reader llamaindex\n",
    "Note: it throws an error when there are empty files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 14\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/examples/../app.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/examples/../code_chunker.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/examples/../code_graph.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/examples/../core/graph.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/examples/../database/snippet_database.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/examples/../database/vector_store.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/examples/../embedding/context_wrapper.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/examples/../embedding/embedding.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/examples/../embedding/llm_adapter.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/examples/../github_interface.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/examples/../openapi_understand.py\n"
     ]
    }
   ],
   "source": [
    "def simple_directory_reader(path: str):\n",
    "    try:\n",
    "        documents = SimpleDirectoryReader(\n",
    "            input_dir=path,\n",
    "            recursive=True,\n",
    "            required_exts=[\".py\"],\n",
    "            exclude=[\n",
    "                \".venv/**\",\n",
    "                \".vscode/**\",\n",
    "                \"**/*.ipynb\"\n",
    "            ],\n",
    "            file_metadata=lambda x: {\"filepath\": x}\n",
    "        ).load_data()\n",
    "        print(f\"Number of documents loaded: {len(documents)}\")\n",
    "        new_docs = []\n",
    "        if len(documents) > 0:\n",
    "            for doc in documents:\n",
    "                if len(doc.text) > 0:\n",
    "                    # bug related to llama_index that happens in empty files too.\n",
    "                    doc.set_content(_remove_non_ascii(doc.text))\n",
    "                    new_docs.append(doc)\n",
    "                    print(f\"...... File path: {doc.metadata.get('filepath')}\")\n",
    "                # print(\"-\" * 50)\n",
    "        return new_docs\n",
    "    except ValueError as e:\n",
    "        if \"No files found\" in str(e):\n",
    "            return []\n",
    "        else:\n",
    "            raise  # Re-raise if it's a different ValueError\n",
    "\n",
    "documents = simple_directory_reader('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codesplitter works with full directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully split into 92 nodes\n",
      "\n",
      "------ Node 0 preview:\n",
      "Text length: 21\n",
      "Content preview: class CodebaseLoader:...\n",
      "\n",
      "------ Node 1 preview:\n",
      "Text length: 1294\n",
      "Content preview: def __init__(self, local_dir=None, github_repo=None):\n",
      "        self.local_dir = local_dir\n",
      "        sel...\n",
      "\n",
      "------ Node 2 preview:\n",
      "Text length: 1138\n",
      "Content preview: def __load_local_codebase(self, directory) -> list[Snippet]:\n",
      "        snippets = []\n",
      "        for filen...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:   \n",
    "    splitter = CodeSplitter(\n",
    "        language=\"python\",\n",
    "        max_chars=1500,\n",
    "        chunk_lines=20,\n",
    "    )\n",
    "    split_nodes = splitter.get_nodes_from_documents(documents)\n",
    "    print(f\"Successfully split into {len(split_nodes)} nodes\")\n",
    "    \n",
    "    # Print first few nodes to verify content\n",
    "    for i, node in enumerate(split_nodes[1:4]):\n",
    "        print(f\"\\n------ Node {i} preview:\")\n",
    "        print(f\"Text length: {len(node.text)}\")\n",
    "        print(f\"Content preview: {node.text[:100]}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {type(e)}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### CodeHierarchy doesn't work with full directory, when recursive is True\n",
    "\n",
    "That's because it doesn't work well with empty files and we have to modify empty files on our own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  /Users/shivanshj/repo-personal-projects/codebase2vec\n",
      "Number of documents loaded: 14\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/app.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/code_chunker.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/code_graph.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/core/graph.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/database/snippet_database.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/database/vector_store.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/embedding/context_wrapper.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/embedding/embedding.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/embedding/llm_adapter.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/github_interface.py\n",
      "...... File path: /Users/shivanshj/repo-personal-projects/codebase2vec/openapi_understand.py\n",
      "Number of documents loaded in /Users/shivanshj/repo-personal-projects/codebase2vec: 11\n",
      "...... code_hierarchy found\n",
      "Node ID: 6a36d6ff-7525-43a5-8de7-eb18a45cad6a\n",
      "Text: import os import streamlit as st import code_chunker from\n",
      "embedding.llm_adapter import LLMAdapter from github_interface import\n",
      "load_github_codebase from embedding.embedding import CodeEmbedding  #\n",
      "Import the CodeEmbedding class from dotenv import load_dotenv  from\n",
      "embedding.context_wrapper import Summarizer from database.vector_store\n",
      "import Vect...\n",
      "------\n",
      "Node ID: 4b203d81-0751-4683-9ac3-9baf3e9b84fd\n",
      "Text: class CodebaseLoader:     def __init__(self, local_dir=None,\n",
      "github_repo=None):         # Code replaced for brevity. See node_id\n",
      "b35ea862-ac60-4a63-b441-3249dda0ed68      def load_codebase(self) ->\n",
      "list[Snippet]:         # Code replaced for brevity. See node_id\n",
      "2211354a-788c-4191-9168-a18c3842bd4b      def __test(self, txt):\n",
      "return txt  ...\n",
      "------\n",
      "Node ID: b35ea862-ac60-4a63-b441-3249dda0ed68\n",
      "Text: def __init__(self, local_dir=None, github_repo=None):\n",
      "self.local_dir = local_dir         self.github_repo = github_repo\n",
      "self.db = SnippetDatabase()         self.repo_id =\n",
      "self.db.make_repo_id(self.github_repo)         self.snippets = []\n",
      "self.directory_structure = None\n",
      "------\n",
      "Node ID: 2211354a-788c-4191-9168-a18c3842bd4b\n",
      "Text: def load_codebase(self) -> list[Snippet]:         if\n",
      "self.db.repo_exists(self.repo_id):             print (\"CodebaseLoader\n",
      ":  repo exists in relational DB\")             return\n",
      "self.db.load_snippets(self.repo_id)                  if\n",
      "self.github_repo:             self.snippets =\n",
      "load_github_codebase(self.github_repo)         elif self.local_dir:\n",
      "...\n",
      "------\n",
      "Node ID: 7e84f9b2-1b22-41e3-98b1-70534a4876cd\n",
      "Text: def extract_dir_structure(self, snippets: list[Snippet]):\n",
      "if dir := self.db.get_repo_dir_structure(self.repo_id):\n",
      "print (\"CodebaseLoader :  dir exists in relational DB\")\n",
      "return dir         dir_structure = '\\n'         for snippet in\n",
      "snippets:             dir_structure += snippet.file_path\n",
      "dir_structure...\n",
      "------\n",
      "Node ID: 28adfb79-d64c-46f5-b1aa-5c26c761b292\n",
      "Text: def __load_local_codebase(self, directory) -> list[Snippet]:\n",
      "snippets = []         for filename in os.listdir(directory):\n",
      "if filename.startswith('.'):                 continue\n",
      "filepath = os.path.join(directory, filename)             if\n",
      "os.path.isdir(filepath):\n",
      "snippets.extend(self.__load_local_code...\n",
      "------\n",
      "Node ID: fc245a91-a80d-41d0-8b9c-8ba0bcfe41cf\n",
      "Text: def is_valid_file(filepath):         IGNORED_FILES = [\"package-\n",
      "lock.json\", \"yarn.lock\", \"poetry.lock\"]         ALLOWED_EXTENSIONS =\n",
      "[\".py\", \".tsx\"]         return (not any(ignored in filepath for\n",
      "ignored in IGNORED_FILES) and\n",
      "any(filepath.endswith(ext) for ext in ALLOWED_EXTENSIONS))\n",
      "------\n",
      "Node ID: 1099d0f1-75eb-42b1-961c-2a5ffe9c59a9\n",
      "Text: def main():     st.title(\"Codebase Ingestion and Embedding\n",
      "Generation\")      # Initialize session state     if 'step' not in\n",
      "st.session_state:         st.session_state.step = 1     if 'input1'\n",
      "not in st.session_state:         st.session_state.input1 = \"\"\n",
      "vector_store = VectorStore(collection_name=\"dev_codebase2\",\n",
      "vector_size=1536)     embed...\n",
      "------\n",
      "Node ID: c0f9c2b6-6b0f-4c2a-9e62-448c84dbdc48\n",
      "Text: def make_embeddings():                     if\n",
      "vector_store.does_embedding_exist(repo_id):\n",
      "return                      for snippet in snippets:\n",
      "snippet, file_path = snippet.content, snippet.file_path\n",
      "try:                             code_chunks =\n",
      "code_chunker.chunk_code(snipp...\n",
      "------\n",
      "Node ID: 10b31a93-cd6a-4ce5-86ac-b5a9f1d5d05d\n",
      "Text: def top_matches_from_vector_store(res):                     for\n",
      "x in res[:3]:                         st.markdown(f\"**File:\n",
      "{x.payload['file_path']}**\")                         st.code(f\"Matched\n",
      "Code:\\n{x.payload['code_chunk']}...\\n\", language=\"python\")\n",
      "st.text_area(\"Abstract:\\n\", f\"{x.payload['abstract']}\" )\n",
      "------\n",
      "Node ID: 685ac94f-5d7a-4641-9e20-43647adbb559\n",
      "Text: def generate_code(winning_code_chunk, winning_code_abstract):\n",
      "print ('in generate_code()', dir_structure)                     llm =\n",
      "LLMAdapter()                     user_prompt = \"generate code based on\n",
      "the following function definition, so i know how to use this as an\n",
      "API\"                     user_prompt += winning_code_abst...\n",
      "------\n",
      "Node ID: 8f257fd5-ceec-4042-9d6e-b4b18dc39766\n",
      "Text: # code_chunker.py # Chunking code into smaller chunks for\n",
      "embedding  # We have to chunk and preserve context: which funnction is\n",
      "contained within a class   from __future__ import annotations from\n",
      "dataclasses import dataclass from typing import List, Tuple from\n",
      "whats_that_code.election import guess_language_all_methods  from\n",
      "tree_sitter import No...\n",
      "------\n",
      "Node ID: 87e6db98-b062-4414-b61b-4a09d83284cb\n",
      "Text: def chunk_code(source_code: str) -> List[str]:     code_chunks =\n",
      "[]     language = guess_language_all_methods(source_code)     for\n",
      "chunk in BlockAwareCodeSplitter.split_text(source_code, language):\n",
      "# continue         print (chunk)         code_chunk_str =\n",
      "chunk.extract_lines(source_code)         # print(code_chunk_str)\n",
      "print (\"==...\n",
      "------\n",
      "Node ID: 78e2c764-c717-42b8-86d2-141caa90bcf6\n",
      "Text: class Chunk:     \"\"\"     Representing a slice of a string.\n",
      "Start & End can be bytes or lines depending on how you store it.\n",
      "\"\"\"     start: int = 0     end: int = 0      def __post_init__(self):\n",
      "# Code replaced for brevity. See node_id\n",
      "ab8f98e3-1e95-4e86-84d4-e556cd3960aa      def extract(self, s: str) ->\n",
      "str:         # Grab the c...\n",
      "------\n",
      "Node ID: ab8f98e3-1e95-4e86-84d4-e556cd3960aa\n",
      "Text: def __post_init__(self):         if self.end is None:\n",
      "self.end = self.start\n",
      "------\n",
      "Node ID: 984f4603-521b-4894-9010-0d5550b61ea8\n",
      "Text: def extract(self, s: str) -> str:         # Grab the\n",
      "corresponding substring of string s by bytes         return\n",
      "s[self.start : self.end]\n",
      "------\n",
      "Node ID: e30e4882-4260-4388-bf9e-f196aeb8ed3f\n",
      "Text: def extract_lines(self, s: str) -> str:         # Grab the\n",
      "corresponding substring of string s by lines         return\n",
      "\"\\n\".join(s.splitlines()[self.start : self.end + 1 ])\n",
      "------\n",
      "Node ID: e55bebf3-9d46-490b-96b3-769639a8a3b0\n",
      "Text: def __add__(self, other: Chunk | int) -> Chunk:         # e.g.\n",
      "Chunk(1, 2) + Chunk(2, 4) = Chunk(1, 4) (concatenation)         #\n",
      "There are no safety checks: Chunk(a, b) + Chunk(c, d) = Chunk(a, d)\n",
      "# and there are no requirements for b = c.          if\n",
      "isinstance(other, int):             return Chunk(self.start + other,\n",
      "self.end + other) ...\n",
      "------\n",
      "Node ID: ce13ac04-6284-4fdc-8650-519df488be72\n",
      "Text: def __len__(self) -> int:         # i.e. Chunk(a, b) = b - a\n",
      "return self.end - self.start\n",
      "------\n",
      "Node ID: 168125e5-5149-45ae-8073-97d0182f1cc7\n",
      "Text: def _get_line_number_from_char_index(index: int, source_code:\n",
      "str) -> int:     \"\"\"     Get the line number from a given character\n",
      "index in the source code.     It iterates through the lines of the\n",
      "source code, keeping track of the total characters     processed,\n",
      "until it finds the line containing the specified index.      Args:\n",
      "index (in...\n",
      "------\n",
      "Node ID: b445fddb-551d-491c-a480-d68f894c2f15\n",
      "Text: class TextChunker:     @staticmethod     def\n",
      "split_text(source_code: str, language: str) -> List[Chunk]:         #\n",
      "Code replaced for brevity. See node_id\n",
      "c35cc1da-d3fe-4015-86b5-ec9c1042522d\n",
      "@staticmethod     def _chunk_node(node: Node, MAX_CHARS: int = 600) ->\n",
      "List[Chunk]:         # 1. Recursively form chunks based on th...\n",
      "------\n",
      "Node ID: c35cc1da-d3fe-4015-86b5-ec9c1042522d\n",
      "Text: def split_text(source_code: str, language: str) -> List[Chunk]:\n",
      "\"\"\"Split incoming code and return chunks using the AST.\"\"\"\n",
      "parser = get_parser(language)         tree =\n",
      "parser.parse(source_code.encode(\"utf8\"))         chunks =\n",
      "TextChunker._chunk_node(tree.root_node)         print (chunks,\n",
      "end=\"\\n\\n\")          # 2. Filling in the ...\n",
      "------\n",
      "Node ID: 579f62af-1d54-42fa-acf7-85091e7ea908\n",
      "Text: def _chunk_node(node: Node, MAX_CHARS: int = 600) ->\n",
      "List[Chunk]:         # 1. Recursively form chunks based on the last\n",
      "post (https://docs.sweep.dev/blogs/chunking-2m-files)         chunks:\n",
      "list[Chunk] = []         current_chunk: Chunk = Chunk(node.start_byte,\n",
      "node.start_byte)          for child in node.children:             if\n",
      "child.end_byte -...\n",
      "------\n",
      "Node ID: 9a47e719-8ae4-4dc2-bf1e-31c1bd24b861\n",
      "Text: def __coalesce_chunks(chunks: list[Chunk], source_code: str,\n",
      "coalesce: int = 50) -> list[Chunk]:         \"\"\"         Coalesce small\n",
      "chunks into larger ones.          Args:             chunks\n",
      "(list[Chunk]): A list of Chunk objects representing the initial\n",
      "chunks.             source_code (str): The original source code\n",
      "string.             coalesce...\n",
      "------\n",
      "Node ID: 1bba1178-1647-4607-94a6-27f24599f126\n",
      "Text: class BlockAwareCodeSplitter:     \"\"\"     Split code into chunks\n",
      "while preserving the blocks (functions, classes, etc.) that contain\n",
      "them.     \"\"\"     def __init__(self, overlap_lines: int = 2):\n",
      "# Code replaced for brevity. See node_id\n",
      "8db4509b-a97d-4291-9659-9feff80cec94      @staticmethod     def\n",
      "split_text(source_code: str, language: ...\n",
      "------\n",
      "Node ID: 8db4509b-a97d-4291-9659-9feff80cec94\n",
      "Text: def __init__(self, overlap_lines: int = 2):\n",
      "self.overlap_lines = overlap_lines\n",
      "------\n",
      "Node ID: b010e50b-2c29-44b8-8478-0942b7c14471\n",
      "Text: def split_text(source_code: str, language: str) ->\n",
      "List[Tuple[str, str]]:         parser = get_parser(language)\n",
      "tree = parser.parse(source_code.encode(\"utf8\"))\n",
      "blocks = BlockAwareCodeSplitter._extract_blocks(tree.root_node)\n",
      "line_chunks = []         for b in blocks:             chunk = b.span\n",
      "...\n",
      "------\n",
      "Node ID: c82603a5-23e4-4c80-ba53-ed7c7a0fac38\n",
      "Text: def _extract_blocks(node: Node) -> List[MyBlock]:         blocks\n",
      "= []         for child in node.children:             try:\n",
      "block_type = BlockAwareCodeSplitter._get_block_type(child)\n",
      "block_name = BlockAwareCodeSplitter._get_block_name(child, child.type)\n",
      "block_span = Chunk(child.start_byte, child.end...\n",
      "------\n",
      "Node ID: 5a2959cb-ad37-4d0b-870e-cc27bb9748ad\n",
      "Text: def _get_block_type(node: Node) -> str:         try:\n",
      "# These are the types we are interested in             if node.type in\n",
      "[\"function_definition\",\"function_declaration\",\"arrow_function\",\"method\n",
      "_definition\"]:                 return 'function'             elif\n",
      "node.type in [\"class_definition\", \"class_declaration\"]:\n",
      "ret...\n",
      "------\n",
      "Node ID: 24d394b6-b42d-465f-9df1-1c0f8b7a818b\n",
      "Text: def _get_block_name(node: Node, block_type: str) -> str:\n",
      "try:                      if block_type in ['function_definition',\n",
      "'class_definition', 'method_definition']:                 name_node =\n",
      "node.child_by_field_name('name')             elif block_type ==\n",
      "'impl_item':                 name_node =\n",
      "node.child_by_field_name('trait') or nod...\n",
      "------\n",
      "Node ID: 89ac7159-92e8-407c-bc6b-664b01c578e1\n",
      "Text: def _overlaps(self, func_span, chunk_span):         return\n",
      "(func_span.start <= chunk_span.end and func_span.end >=\n",
      "chunk_span.start)\n",
      "------\n",
      "Node ID: e387d624-709b-4627-9247-887c2a215465\n",
      "Text: class TestBlockAwareCodeSplitter(unittest.TestCase):     def\n",
      "setUp(self):         self.splitter = BlockAwareCodeSplitter()      def\n",
      "test_split_text(self):         # Code replaced for brevity. See\n",
      "node_id 8fe221f1-946c-48f2-bd2b-64415a626c90      def\n",
      "test_get_block_type(self):         # Code replaced for brevity. See\n",
      "node_id 8f6d8a89-277c-481b-8c...\n",
      "------\n",
      "Node ID: 8fe221f1-946c-48f2-bd2b-64415a626c90\n",
      "Text: def test_split_text(self):         code = \"\"\" def function1():\n",
      "print(\"Hello\")  class TestClass:     def method1(self):         return\n",
      "\"World\"      def method2(self):         return \"hell\"  def\n",
      "function2():     return 42         \"\"\"         chunks =\n",
      "list(self.splitter.split_text(code, 'python'))\n",
      "self.assertEqual(len(chunks), ...\n",
      "------\n",
      "Node ID: 8f6d8a89-277c-481b-8c1b-f10e4fefabad\n",
      "Text: def test_get_block_type(self):         parser =\n",
      "get_parser('python')         tree = parser.parse(bytes(\"def\n",
      "test_function():\\n    pass\", 'utf8'))         root_node =\n",
      "tree.root_node         function_node = root_node.children[0]\n",
      "block_type = self.splitter._get_block_type(function_node)\n",
      "self.assertEqual(block_type, 'function')\n",
      "------\n",
      "Node ID: f333b0af-fffe-4cad-9244-95c355ad0a41\n",
      "Text: def test_get_block_name(self):         parser =\n",
      "get_parser('python')         tree = parser.parse(bytes(\"def\n",
      "test_function():\\n    pass\", 'utf8'))         root_node =\n",
      "tree.root_node         function_node = root_node.children[0]\n",
      "block_name = self.splitter._get_block_name(function_node,\n",
      "'function_definition')         self.assertEqu...\n",
      "------\n",
      "Node ID: 9aabc195-e24c-413f-b063-37ce9781bb35\n",
      "Text: def test_overlaps(self):         chunk1 = Chunk(0, 10)\n",
      "chunk2 = Chunk(5, 15)         chunk3 = Chunk(11, 20)\n",
      "self.assertTrue(self.splitter._overlaps(chunk1, chunk2))\n",
      "self.assertFalse(self.splitter._overlaps(chunk1, chunk3))\n",
      "------\n",
      "Node ID: ddcdb525-bd1e-44cf-83cc-840180e0c96c\n",
      "Text: class Relation(enum.Enum):     # Code replaced for brevity. See\n",
      "node_id ba9864be-bc40-4deb-a43e-cc01c244c89c    class Node:     # Code\n",
      "replaced for brevity. See node_id ceb22e93-c93f-4d71-90b7-3031a47a7d96\n",
      "------\n",
      "Node ID: ba9864be-bc40-4deb-a43e-cc01c244c89c\n",
      "Text: class Relation(enum.Enum):     HAS_FUNCTION = 'HAS_FUNCTION'\n",
      "HAS_OBJECT_INITIALIZATION = 'HAS_OBJECT'\n",
      "------\n",
      "Node ID: ceb22e93-c93f-4d71-90b7-3031a47a7d96\n",
      "Text: class Node:     is_sdk_function: bool     children_nodes:\n",
      "List[Node]     parent_nodes: List[Node]\n",
      "------\n",
      "Node ID: dfd54f4f-8c89-420a-aa3b-be64aefda69e\n",
      "Text: from llama_index.core import SimpleDirectoryReader\n",
      "------\n",
      "Node ID: d55ee581-40ca-410d-85d5-ec4bf74c3132\n",
      "Text: import pickle import os import sys from dataclasses import\n",
      "dataclass   @dataclass class Snippet:     content: str     file_path:\n",
      "str   class SnippetDatabase:     # Code replaced for brevity. See\n",
      "node_id b8bd71a6-aa9e-42f9-9bda-93a749163b1a        import unittest\n",
      "class TestSnippetDatabase(unittest.TestCase):     # Code replaced for\n",
      "brevity. See ...\n",
      "------\n",
      "Node ID: b8bd71a6-aa9e-42f9-9bda-93a749163b1a\n",
      "Text: class SnippetDatabase:     def __init__(self):         # Code\n",
      "replaced for brevity. See node_id 7326f6d0-af86-4ccb-84eb-84399a3fcb84\n",
      "def load_snippets(self, repo_id=None):         # Code replaced for\n",
      "brevity. See node_id b403415f-ea93-45b8-8490-7997ee131ab9          def\n",
      "get_repo_dir_structure(self, repo_id: str) -> str:         # Code ...\n",
      "------\n",
      "Node ID: 7326f6d0-af86-4ccb-84eb-84399a3fcb84\n",
      "Text: def __init__(self):         self.db_folder = \"database\"\n",
      "self.db_file = os.path.join(self.db_folder, \"snippets.pkl\")\n",
      "self.all_snippets = {}          os.makedirs(self.db_folder,\n",
      "exist_ok=True)                  if not os.path.exists(self.db_file):\n",
      "print('Creating empty database file')             with\n",
      "open(self.db_file, ...\n",
      "------\n",
      "Node ID: b403415f-ea93-45b8-8490-7997ee131ab9\n",
      "Text: def load_snippets(self, repo_id=None):         \"\"\"Load snippets\n",
      "from the pickle file.\"\"\"         if os.path.exists(self.db_file):\n",
      "with open(self.db_file, 'rb') as f:                 all_snippets =\n",
      "pickle.load(f)                 repo_db = all_snippets.get(repo_id, {})\n",
      "return repo_db['snippets']         return []\n",
      "------\n",
      "Node ID: 6c00c96a-42af-4598-9a55-32ab3487f4cb\n",
      "Text: def get_repo_dir_structure(self, repo_id: str) -> str:\n",
      "if os.path.exists(self.db_file):             with open(self.db_file,\n",
      "'rb') as f:                 all_snippets = pickle.load(f)\n",
      "repo_db = all_snippets.get(repo_id, {})                  return\n",
      "repo_db['dir_structure'] if 'dir_structure' in repo_db else ''\n",
      "return ''\n",
      "------\n",
      "Node ID: 0030868f-d980-4ee3-91ed-65420b67b541\n",
      "Text: def make_repo_id(self, repo_input: str) -> str:         if\n",
      "repo_input.startswith(\"http\"):             # It's a GitHub URL\n",
      "parts = repo_input.split(\"/\")             if len(parts) >= 2:\n",
      "return f\"{parts[-2]}/{parts[-1]}\"         else:             abs_path =\n",
      "os.path.abspath(repo_input)             return os.path.basename(...\n",
      "------\n",
      "Node ID: 0d4fc6ee-d0a7-45de-b551-70f228f15c99\n",
      "Text: def save_snippet(self, repo_id: str, snippet: Snippet):\n",
      "\"\"\"Save a snippet to the database.\"\"\"         if repo_id not in\n",
      "self.all_snippets:             self.all_snippets[repo_id] = {}\n",
      "self.all_snippets[repo_id]['snippets'] = []\n",
      "self.all_snippets[repo_id]['snippets'].append(snippet)         with\n",
      "open(self.db_file, 'wb')...\n",
      "------\n",
      "Node ID: 68932ede-5641-42a5-af2b-773911bbdb62\n",
      "Text: def save_repo_dir_structure(self, repo_id: str, dir_structure:\n",
      "str):         if repo_id not in self.all_snippets:\n",
      "self.all_snippets[repo_id] = {}\n",
      "self.all_snippets[repo_id]['dir_structure'] = dir_structure\n",
      "------\n",
      "Node ID: 0264cca7-5b0d-4acd-8a37-2e5c49772701\n",
      "Text: def repo_exists(self, repo_id: str):         \"\"\"Check if the\n",
      "repository already exists in the database.\"\"\"         return repo_id\n",
      "in self.all_snippets and len(self.all_snippets[repo_id]) > 0\n",
      "------\n",
      "Node ID: 21123f9e-e98e-4322-8601-2ed114aec309\n",
      "Text: class TestSnippetDatabase(unittest.TestCase):     def\n",
      "setUp(self):         # Code replaced for brevity. See node_id\n",
      "4c359f35-1f16-4727-85d7-0531f8d4e0fb      def\n",
      "test_make_repo_id_github_url(self):         # Code replaced for\n",
      "brevity. See node_id 95362521-d838-43b5-bf4d-24390af9e004        def\n",
      "test_make_repo_id_local_path(self):         # Test l...\n",
      "------\n",
      "Node ID: 4c359f35-1f16-4727-85d7-0531f8d4e0fb\n",
      "Text: def setUp(self):         self.db = SnippetDatabase()  # Assuming\n",
      "SnippetDatabase is the class name\n",
      "------\n",
      "Node ID: 95362521-d838-43b5-bf4d-24390af9e004\n",
      "Text: def test_make_repo_id_github_url(self):         \"\"\"Test the\n",
      "make_repo_id method.\"\"\"         # Test GitHub URL         github_url =\n",
      "\"https://github.com/username/repo-name\"         expected_id =\n",
      "\"username/repo-name\"\n",
      "self.assertEqual(self.db.make_repo_id(github_url), expected_id)\n",
      "------\n",
      "Node ID: 038ab16e-adb4-4082-ac87-3005aa914bb4\n",
      "Text: def test_make_repo_id_local_path(self):         # Test local\n",
      "directory path         local_path = \"./my-project\"\n",
      "self.assertEqual(self.db.make_repo_id(local_path), \"my-project\")\n",
      "# Test edge case: URL with no repository name         edge_case_url =\n",
      "\"./\"         self.assertEqual(self.db.make_repo_id(edge_case_url),\n",
      "\"codebase2vec\")\n",
      "------\n",
      "Node ID: 05868c1d-6bcc-43bf-a16c-faefdbd8bd5d\n",
      "Text: def test_make_repo_id_unrecognized(self):         # Test\n",
      "unrecognized input         unrecognized = \"not_a_url_or_path\"\n",
      "self.assertEqual(self.db.make_repo_id(unrecognized), unrecognized)\n",
      "------\n",
      "Node ID: eb8b1de6-c9d5-47c6-8db3-4b0cf333af9a\n",
      "Text: # Import necessary libraries import os from dataclasses import\n",
      "dataclass from qdrant_client import QdrantClient from\n",
      "qdrant_client.http.models import PointStruct, PointIdsList,\n",
      "FilterSelector, Filter, Distance,VectorParams, FieldCondition,\n",
      "MatchValue   @dataclass class VectorNode:     # Code replaced for\n",
      "brevity. See node_id 8a21da95-a80b-42a9-a...\n",
      "------\n",
      "Node ID: 8a21da95-a80b-42a9-a7f3-f380762b8355\n",
      "Text: class VectorNode:     id: str     embedding: list     metadata:\n",
      "dict      def __init__(self, embedding: list, metadata: dict, id: str\n",
      "= None):         # Code replaced for brevity. See node_id\n",
      "0ca31260-614f-4b3b-92fa-d51786fd04b0\n",
      "------\n",
      "Node ID: 0ca31260-614f-4b3b-92fa-d51786fd04b0\n",
      "Text: def __init__(self, embedding: list, metadata: dict, id: str =\n",
      "None):         import uuid         self.id = id if id is not None else\n",
      "str(uuid.uuid4())         self.embedding = embedding\n",
      "self.metadata = metadata\n",
      "------\n",
      "Node ID: b3bebaba-b508-448e-be29-34948ba24b43\n",
      "Text: class VectorStore:     def __init__(self, collection_name: str,\n",
      "vector_size: int = 128, max_retries: int = 3, retry_delay: int = 2):\n",
      "# Initialize Qdrant client         # Code replaced for brevity. See\n",
      "node_id 4f7f3146-01fc-4f60-9f1e-ede766a42942       def\n",
      "_connect_with_retry(self, max_retries, retry_delay):         # Code\n",
      "replaced for br...\n",
      "------\n",
      "Node ID: 4f7f3146-01fc-4f60-9f1e-ede766a42942\n",
      "Text: def __init__(self, collection_name: str, vector_size: int = 128,\n",
      "max_retries: int = 3, retry_delay: int = 2):         # Initialize\n",
      "Qdrant client         self.HOST = os.getenv(\"QDRANT_HOST\",\n",
      "\"localhost\")         self.PORT = os.getenv(\"QDRANT_PORT\", \"6333\")\n",
      "self.QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")         print\n",
      "(self.HOST, self.PO...\n",
      "------\n",
      "Node ID: bfd0375e-fef5-4361-bd51-48b76194f160\n",
      "Text: def _connect_with_retry(self, max_retries, retry_delay):\n",
      "import time         for attempt in range(max_retries):\n",
      "try:                 # Attempt to connect                 client =\n",
      "QdrantClient(self.HOST, port=self.PORT, api_key=self.QDRANT_API_KEY)\n",
      "# Test the connection                 client.get_collections() ...\n",
      "------\n",
      "Node ID: 31d43c7e-6fc7-4aac-8550-f744efd12e11\n",
      "Text: def does_embedding_exist(self, repo_id):         # Check if\n",
      "embeddings already exist         search_result = self.client.scroll(\n",
      "collection_name=self.collection_name,\n",
      "scroll_filter=Filter(                 must=[\n",
      "FieldCondition(                         key=\"repo_id\",\n",
      "match=MatchV...\n",
      "------\n",
      "Node ID: 44230df2-357d-4efc-8920-47a7ae9a6317\n",
      "Text: def add_vectors(self, nodes: list[VectorNode], repo_id=1):\n",
      "points = []         for node in nodes:\n",
      "points.append(PointStruct(id=node.id, vector=node.embedding,\n",
      "payload=node.metadata))\n",
      "self.client.upsert(collection_name=self.collection_name,\n",
      "points=points)\n",
      "------\n",
      "Node ID: 7139f927-27e5-4c9b-b6fd-b847f9a6722f\n",
      "Text: def search(self, query_vector: list, limit: int = 10):         #\n",
      "Search for similar vectors in the Qdrant collection         return\n",
      "self.client.search(collection_name=self.collection_name,\n",
      "query_vector=query_vector, limit=limit)\n",
      "------\n",
      "Node ID: ac34a6b8-1b22-40b0-835a-825f27b5df14\n",
      "Text: def get_vectors_by_id(self, point_ids: list):         # Search\n",
      "for similar vectors in the Qdrant collection         return\n",
      "self.client.retrieve(collection_name=self.collection_name,\n",
      "ids=point_ids)\n",
      "------\n",
      "Node ID: f9f3395f-d09c-456b-8885-35e5417f267e\n",
      "Text: def delete_nodes(self, node_ids: list[str]):\n",
      "self.client.delete(             collection_name=self.collection_name,\n",
      "points_selector=PointIdsList(                 points=node_ids\n",
      ")         )\n",
      "------\n",
      "Node ID: c4bc1900-47fb-4b29-aeab-4c5037e6825a\n",
      "Text: def _get_collection(self, collection_name: str, vector_size:\n",
      "int):         try:                          collection_info =\n",
      "self.client.get_collection(collection_name)\n",
      "print(f\"Connected to existing collection: {collection_name}\")\n",
      "# Optionally, you can verify the collection parameters here\n",
      "if collection_info.co...\n",
      "------\n",
      "Node ID: d93570a7-637a-4ad4-97c4-8c06ad635ee2\n",
      "Text: def _create_collection(self, collection_name: str, vector_size:\n",
      "int):         try:             self.client.recreate_collection(\n",
      "collection_name=collection_name,\n",
      "vectors_config=VectorParams(size=vector_size,\n",
      "distance=Distance.COSINE)             )         except Exception as e:\n",
      "raise RuntimeError(f\"Fail...\n",
      "------\n",
      "Node ID: c3470a49-778e-432a-9dda-ee64dd5202dc\n",
      "Text: def delete_all_nodes(self):         \"\"\"         Delete all nodes\n",
      "from the collection.         \"\"\"         self.client.delete(\n",
      "collection_name=self.collection_name,\n",
      "points_selector=FilterSelector(                 filter=Filter(\n",
      "must=[],                     must_not=[],\n",
      "should=[]     ...\n",
      "------\n",
      "Node ID: 9587eedf-e286-4250-8f7e-b3224a31fcbf\n",
      "Text: class TestVectorStore(unittest.TestCase):     def setUp(self):\n",
      "# Code replaced for brevity. See node_id 4a6e35da-\n",
      "daa5-4c6a-b9c1-ddd26c91ccfd      def test_vector_connection(self):\n",
      "# Create test nodes         # Code replaced for brevity. See node_id\n",
      "a6ef082e-d76f-4309-ba3c-34b07327cffe      def\n",
      "test_vector_search_by_id(self):     ...\n",
      "------\n",
      "Node ID: 4a6e35da-daa5-4c6a-b9c1-ddd26c91ccfd\n",
      "Text: def setUp(self):         from dotenv import load_dotenv\n",
      "load_dotenv()          self.vector_size = 768\n",
      "self.collection_name = \"dev_codebase\"         self.vs =\n",
      "VectorStore(self.collection_name, self.vector_size)\n",
      "------\n",
      "Node ID: a6ef082e-d76f-4309-ba3c-34b07327cffe\n",
      "Text: def test_vector_connection(self):         # Create test nodes\n",
      "id1, id2, id3 = str(uuid.uuid4()), str(uuid.uuid4()),\n",
      "str(uuid.uuid4())         test_nodes = [\n",
      "VectorNode(id=id1,\n",
      "embedding=np.random.rand(self.vector_size).tolist(), metadata={\"text\":\n",
      "\"Test document 1\"}),             VectorNode(id=id2,\n",
      "embedding=np.random.rand(sel...\n",
      "------\n",
      "Node ID: 51a5c181-0f55-4a6e-b13b-a1943a713ded\n",
      "Text: def test_vector_search_by_id(self):         # Create test nodes\n",
      "id1, id2, id3 = 1,2,3         test_nodes = [\n",
      "VectorNode(id=id1,\n",
      "embedding=np.random.rand(self.vector_size).tolist(), metadata={\"text\":\n",
      "\"Test document 1\"}),             VectorNode(id=id2,\n",
      "embedding=np.random.rand(self.vector_size).tolist(), metadata={\"text\":\n",
      "\"Test...\n",
      "------\n",
      "Node ID: 88ab95af-b014-4cb3-8c37-f7aa73eb3ed4\n",
      "Text: def tearDown(self):         # Clean up the test collection\n",
      "self.vs.delete_all_nodes()\n",
      "------\n",
      "Node ID: 1ac09b0a-7cd4-45e1-b972-48c21508802a\n",
      "Text: from typing import List, Tuple, Dict import textwrap import ast\n",
      "from embedding.llm_adapter import LLMAdapter   function_summary_prompt\n",
      "= \"\"\" <file_location> {{PATH_TO_FILE}} </file_location>  <target_code>\n",
      "{{CODE_CHUNK}} </target_code>  Analyze the code chunk within the\n",
      "larger file context and provide a concise docstring that includes:  1.\n",
      "The ...\n",
      "------\n",
      "Node ID: bb52ba0f-5198-45c4-bdd8-0703bebfb9a8\n",
      "Text: class Summarizer:      @staticmethod     def\n",
      "context_of_snippet(snippet_text, document_text):         # generates\n",
      "function summary in context         # Code replaced for brevity. See\n",
      "node_id 2a33a60a-bfb9-4828-93c5-3a797752bcb0       @staticmethod\n",
      "def generate_abstract_with_api(file_path: str, block_code: str) ->\n",
      "str:         # Code replaced...\n",
      "------\n",
      "Node ID: 2a33a60a-bfb9-4828-93c5-3a797752bcb0\n",
      "Text: def context_of_snippet(snippet_text, document_text):         #\n",
      "generates function summary in context         pass\n",
      "------\n",
      "Node ID: 13690744-0ec5-4829-80fd-5e14d36e8095\n",
      "Text: def generate_abstract_with_api(file_path: str, block_code: str)\n",
      "-> str:         max_tokens = 4000  # Adjust this based on the model's\n",
      "capabilities         if len(block_code) > max_tokens:\n",
      "block_code = textwrap.shorten(block_code, width=max_tokens,\n",
      "placeholder=\"...\")          system_prompt = \"You are a helpful\n",
      "assistant that generates...\n",
      "------\n",
      "Node ID: d70f3af4-42e8-4de0-ab63-e74440e5faee\n",
      "Text: import litellm import numpy as np from torch import Tensor\n",
      "import torch import torch.nn.functional as F from transformers import\n",
      "AutoTokenizer, AutoModel, AutoModelForCausalLM from\n",
      "sentence_transformers import SentenceTransformer    # MODEL_NAME =\n",
      "'microsoft/unixcoder-base' MODEL_NAME ='all-MiniLM-L6-v2' OPENAI_MODEL\n",
      "= 'text-embedding-3-small'  ...\n",
      "------\n",
      "Node ID: 2741d340-990b-42db-a1e3-3498972ecc4e\n",
      "Text: class CodeEmbedding:     def __init__(self,\n",
      "use_sentence_transformer=False, use_llm=False):         # Code\n",
      "replaced for brevity. See node_id b87f50e6-fb92-4b8b-8d24-b5fbbcf98fcf\n",
      "def generate_embeddings(self, snippet: str) -> list[float]:         #\n",
      "Code replaced for brevity. See node_id\n",
      "fd43bf6f-9dd3-406d-8624-5cacafb57857      # us...\n",
      "------\n",
      "Node ID: b87f50e6-fb92-4b8b-8d24-b5fbbcf98fcf\n",
      "Text: def __init__(self, use_sentence_transformer=False,\n",
      "use_llm=False):                 self.embedding_strategy = None\n",
      "if use_llm:             self.embedding_strategy =\n",
      "LiteLLMStrategy(OPENAI_MODEL)         elif use_sentence_transformer:\n",
      "model = SentenceTransformer(MODEL_NAME)\n",
      "self.embedding_strategy = SentenceTransfor...\n",
      "------\n",
      "Node ID: fd43bf6f-9dd3-406d-8624-5cacafb57857\n",
      "Text: def generate_embeddings(self, snippet: str) -> list[float]:\n",
      "return self.embedding_strategy.generate_embeddings(snippet)\n",
      "------\n",
      "Node ID: 63468c59-e239-420f-9377-21dce158c4ac\n",
      "Text: def find_k_nearest_neighbors(query_embedding, embeddings,\n",
      "top_n=3):         # Convert query_embedding to numpy array if it's not\n",
      "already         \"\"\"Search for most similar texts based on cosine\n",
      "similarity.\"\"\"         similarities =\n",
      "[CodeEmbedding.__cosine_similarity(query_embedding, emb) for emb in\n",
      "embeddings]         # Create a list of tuples (...\n",
      "------\n",
      "Node ID: 558b4beb-51b9-4daa-a420-61b9db06fea3\n",
      "Text: def __cosine_similarity(v1, v2):         # Normalize the vectors\n",
      "# Convert to numpy arrays if they're lists         v1_np =\n",
      "np.array(v1)         v2_np = np.array(v2)         # Check if shapes\n",
      "are the same and 1-dimensional         if v1_np.shape == v2_np.shape\n",
      "and v1_np.ndim == 1:             dot_product = np.dot(v1_np, v2_np)\n",
      "el...\n",
      "------\n",
      "Node ID: 403ffd22-054b-4750-9e36-a157b8f1fa29\n",
      "Text: class EmbeddingStrategy:     def generate_embeddings(self,\n",
      "snippet: str) -> list[float]:         # Code replaced for brevity. See\n",
      "node_id f137af5c-0a7e-4730-8a47-70105217785e\n",
      "------\n",
      "Node ID: f137af5c-0a7e-4730-8a47-70105217785e\n",
      "Text: def generate_embeddings(self, snippet: str) -> list[float]:\n",
      "raise NotImplementedError(\"This method should be implemented by\n",
      "subclasses\")\n",
      "------\n",
      "Node ID: 44abb4cb-286f-4fcd-a241-d179527f357f\n",
      "Text: class SentenceTransformerStrategy(EmbeddingStrategy):     def\n",
      "__init__(self, model):         self.model = model      def\n",
      "generate_embeddings(self, snippet: str) -> list[float]:         # Code\n",
      "replaced for brevity. See node_id 6456e484-c1b1-46bf-bb84-a38919ae35cc\n",
      "------\n",
      "Node ID: 6456e484-c1b1-46bf-bb84-a38919ae35cc\n",
      "Text: def generate_embeddings(self, snippet: str) -> list[float]:\n",
      "embeddings = self.model.encode([snippet])         return\n",
      "embeddings[0].tolist()\n",
      "------\n",
      "Node ID: dedef81f-e855-4454-aed8-f9c7e9d2eae8\n",
      "Text: class LiteLLMStrategy(EmbeddingStrategy):     def __init__(self,\n",
      "model_name):         self.model_name = model_name      def\n",
      "generate_embeddings(self, snippet: str) -> list[float]:         # Code\n",
      "replaced for brevity. See node_id 3ba79807-d000-4e95-8391-52ec921e23f3\n",
      "------\n",
      "Node ID: 3ba79807-d000-4e95-8391-52ec921e23f3\n",
      "Text: def generate_embeddings(self, snippet: str) -> list[float]:\n",
      "response = litellm.embedding(             model=self.model_name,\n",
      "input=snippet         )         return\n",
      "response['data'][0]['embedding']\n",
      "------\n",
      "Node ID: 7952c3cb-725e-4e48-a31f-7f71d2467d47\n",
      "Text: class TokenizerStrategy(EmbeddingStrategy):     def\n",
      "__init__(self, tokenizer, model):         # Code replaced for brevity.\n",
      "See node_id 43f24425-b247-4e7d-8d21-b6d1cb877f65       def\n",
      "generate_embeddings(self, snippet: str) -> list[float]:         # Code\n",
      "replaced for brevity. See node_id aa6dbde0-a2ce-43b3-b914-12baaa4b7e38\n",
      "@staticmethod     ...\n",
      "------\n",
      "Node ID: 43f24425-b247-4e7d-8d21-b6d1cb877f65\n",
      "Text: def __init__(self, tokenizer, model):         self.tokenizer =\n",
      "tokenizer         self.model = model\n",
      "------\n",
      "Node ID: aa6dbde0-a2ce-43b3-b914-12baaa4b7e38\n",
      "Text: def generate_embeddings(self, snippet: str) -> list[float]:\n",
      "inputs = self.tokenizer(snippet, return_tensors='pt',max_length=512,\n",
      "truncation=True)         with torch.no_grad():             outputs =\n",
      "self.model(**inputs)         attention_mask = inputs['attention_mask']\n",
      "embeddings = TokenizerStrategy.average_pool(outputs.last_hidde...\n",
      "------\n",
      "Node ID: 81275e24-6ee1-4bd0-89fa-49468bb46ea8\n",
      "Text: def average_pool(last_hidden_states: Tensor, attention_mask:\n",
      "Tensor) -> Tensor:         \"\"\"         This method computes a weighted\n",
      "average of the token embeddings, where padding tokens are ignored.\n",
      "This creates a fixed-size representation for variable-length input\n",
      "sequences,         which is useful for many downstream tasks.\n",
      "Cr...\n",
      "------\n",
      "Node ID: bc1b1db5-6c48-4a97-9102-8a16274d5c06\n",
      "Text: from litellm import completion   class LLMAdapter:     # Code\n",
      "replaced for brevity. See node_id 010d6068-a0ab-42a9-80c0-5ad3316dfed3\n",
      "------\n",
      "Node ID: 010d6068-a0ab-42a9-80c0-5ad3316dfed3\n",
      "Text: class LLMAdapter:     def __init__(self,\n",
      "model_name=\"gpt-3.5-turbo\"):         # Code replaced for brevity. See\n",
      "node_id b5255bc1-a541-434f-88bc-aa62062a55fd       def\n",
      "chat_completion(self, user_prompt, system_prompt):         # Code\n",
      "replaced for brevity. See node_id 94257da1-b020-4a54-a66c-ccb803caedb1\n",
      "------\n",
      "Node ID: b5255bc1-a541-434f-88bc-aa62062a55fd\n",
      "Text: def __init__(self, model_name=\"gpt-3.5-turbo\"):\n",
      "self.model_name = model_name\n",
      "------\n",
      "Node ID: 94257da1-b020-4a54-a66c-ccb803caedb1\n",
      "Text: def chat_completion(self, user_prompt, system_prompt):\n",
      "try:             messages = [                 {\"content\":\n",
      "system_prompt, \"role\": \"system\"},                 {\"content\":\n",
      "user_prompt, \"role\": \"user\"},             ]             response =\n",
      "completion(model=self.model_name, messages=messages)\n",
      "return response['choices'][0]['m...\n",
      "------\n",
      "Node ID: 26881405-d955-4c73-a612-2ad118816f64\n",
      "Text: from dataclasses import dataclass import requests from\n",
      "database.snippet_database import SnippetDatabase, Snippet    db =\n",
      "SnippetDatabase() ALLOWED_EXTENSIONS = [\".py\", \".tsx\"]  def\n",
      "fetch_github_repo_contents(repo_url, subdirectory=''):     # Code\n",
      "replaced for brevity. See node_id c62448fb-416d-4af8-91f6-fa249f668937\n",
      "def load_github_codebase(r...\n",
      "------\n",
      "Node ID: c62448fb-416d-4af8-91f6-fa249f668937\n",
      "Text: def fetch_github_repo_contents(repo_url, subdirectory=''):\n",
      "\"\"\"Fetch the contents of a GitHub repository.\"\"\"     api_url =\n",
      "f\"https://api.github.com/repos/{repo_url}/contents/{subdirectory}\"\n",
      "response = requests.get(api_url)     # response.raise_for_status()  #\n",
      "Raise an error for bad responses     return response.json()\n",
      "------\n",
      "Node ID: 46557536-56ad-4d35-b42e-0c4e251d0031\n",
      "Text: def load_github_codebase(repo_url, subdirectory='') ->\n",
      "list[Snippet]:     \"\"\"Load codebase from GitHub repository.\"\"\"\n",
      "snippets, repo_id = [], db.make_repo_id(repo_url)      try:\n",
      "contents = fetch_github_repo_contents(repo_url, subdirectory)\n",
      "snippets = []                  import pprint         pp =\n",
      "pprint.PrettyPrinter(indent=...\n",
      "------\n",
      "Node ID: a63bb391-b78d-4bf5-8c36-b22f51089398\n",
      "Text: from dataclasses import dataclass import yaml from yaml import\n",
      "YAMLError import numpy as np from embedding.embedding import\n",
      "CodeEmbedding import os    @dataclass class OpenAPIEmbedding:     #\n",
      "Code replaced for brevity. See node_id 395bdd1c-c6e9-42de-\n",
      "ab28-700809b39358    class OpenAPISpecHandler:     # Code replaced for\n",
      "brevity. See node_id 3e85c...\n",
      "------\n",
      "Node ID: 395bdd1c-c6e9-42de-ab28-700809b39358\n",
      "Text: class OpenAPIEmbedding:     path: str     method: str\n",
      "operation: dict     embedding: list[float]\n",
      "------\n",
      "Node ID: 3e85c8ce-79e9-4a6a-8ebd-28d2c7eeb7f6\n",
      "Text: class OpenAPISpecHandler:     def __init__(self,\n",
      "code_embedding_obj: CodeEmbedding):         # Code replaced for\n",
      "brevity. See node_id 6fbf01bc-44a8-4e49-8306-e00498b306bb      def\n",
      "_load_openapi_spec(self, spec_path):         # Code replaced for\n",
      "brevity. See node_id ce6ba192-5bd1-478c-a577-7351975714c0          def\n",
      "generate_embeddings(self):     ...\n",
      "------\n",
      "Node ID: 6fbf01bc-44a8-4e49-8306-e00498b306bb\n",
      "Text: def __init__(self, code_embedding_obj: CodeEmbedding):\n",
      "self.specs = {}         self.code_embedding_obj = code_embedding_obj\n",
      "self.openapi_embeddings = []\n",
      "------\n",
      "Node ID: ce6ba192-5bd1-478c-a577-7351975714c0\n",
      "Text: def _load_openapi_spec(self, spec_path):         \"\"\"Load the\n",
      "OpenAPI specification from a YAML file.\"\"\"         if\n",
      "self.specs.get(spec_path) is not None:             return\n",
      "self.specs[spec_path]         if not spec_path.endswith(('.yaml',\n",
      "'.yml')):             print(f\"Error: {spec_path} is not a YAML file.\")\n",
      "return None         try: ...\n",
      "------\n",
      "Node ID: 7a2902d1-b800-419c-8cfb-98c1cf79777c\n",
      "Text: def generate_embeddings(self):         for spec_path, _ in\n",
      "self.specs:             self._create_endpoint_embeddings(spec_path)\n",
      "------\n",
      "Node ID: 5abf351b-f71e-4c1e-b1d1-3b249279a3c6\n",
      "Text: def _create_endpoint_embeddings(self, spec_path) ->\n",
      "list[OpenAPIEmbedding]:         \"\"\"Create embeddings for each endpoint\n",
      "in an OpenAPI spec file.\"\"\"         spec =\n",
      "self._load_openapi_spec(spec_path)         for path, path_item in\n",
      "spec['paths'].items():             for method, operation in\n",
      "path_item.items():                 description = self._...\n",
      "------\n",
      "Node ID: b2048df7-9292-41dc-bf4d-74af000ff107\n",
      "Text: def find_endpoint_with_query(self, user_query):         \"\"\"Find\n",
      "the best matching endpoint using embeddings.\"\"\"         if not\n",
      "self.openapi_embeddings:             return None         results = []\n",
      "user_embedding =\n",
      "self.code_embedding_obj.generate_embeddings(user_query)\n",
      "embeddings = [oe.embedding for oe in self.openapi_embeddings]...\n",
      "------\n",
      "Node ID: d9680915-dcbe-4e33-90d9-5cb34657912b\n",
      "Text: def _get_endpoint_description(self, path, method, operation):\n",
      "return f\"{method.upper()} {path}: {operation.get('summary', '')} ,\n",
      "{operation.get('description', '')}\"\n",
      "------\n",
      "Node ID: a0fe9a80-db18-4b64-b96d-5b798cdc8b06\n",
      "Text: def is_openapi_spec(spec):     \"\"\"     Check if the given\n",
      "specification is a valid OpenAPI specification.          :param spec:\n",
      "A dictionary containing the parsed YAML/JSON content     :return:\n",
      "Boolean indicating whether it's a valid OpenAPI spec     \"\"\"     #\n",
      "Check for required OpenAPI fields     if not isinstance(spec, dict):\n",
      "return Fa...\n",
      "------\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "def load_and_split_code(path: Path) -> list[NodeWithScore]:\n",
    "    print ('Path: ', str(path))\n",
    "    if path.is_dir() and _skip_directory(path):\n",
    "        print ('skipping')\n",
    "        return []\n",
    "    # -- 1. Get documents in directory\n",
    "    documents = simple_directory_reader(str(path))\n",
    "    if not len(documents) > 0:\n",
    "        return []\n",
    "    print(f\"Number of documents loaded in {path}: {len(documents)}\") \n",
    "    # -- 2. Split the documents into nodes\n",
    "    code_hierarchy = CodeHierarchyNodeParser(\n",
    "        language=\"python\",\n",
    "        code_splitter=CodeSplitter(language=\"python\", max_chars=1000, chunk_lines=10),\n",
    "    )\n",
    "    print ('...... code_hierarchy found')\n",
    "    split_nodes = []\n",
    "    # try:\n",
    "    split_nodes = code_hierarchy.get_nodes_from_documents(documents)\n",
    "    print (len(split_nodes))\n",
    "    # except Exception as e:\n",
    "    #     print ('Exception', e, e.traceback())\n",
    "    #     return []\n",
    "    # -- 3. Recursively traverse all directories and combine all splitnodes\n",
    "    # for sub_path in path.iterdir():\n",
    "    #     if sub_path.is_dir():\n",
    "    #         split_nodes.extend(load_and_split_code(sub_path))\n",
    "    return split_nodes\n",
    "\n",
    "\n",
    "split_nodes = load_and_split_code(Path('../').resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Demo of agent pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.packs.code_hierarchy import CodeHierarchyAgentPack\n",
    "llm = OpenAI(model=\"gpt-4\", temperature=0.2)\n",
    "pack = CodeHierarchyAgentPack(split_nodes=split_nodes, llm=llm)\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 4000\n",
    "print_tail(\n",
    "    pack.run(\n",
    "        \"How does the Codebaseloader class from work? Provide specific implementation details.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Exploreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import os\n",
       "import streamlit as st\n",
       "import code_chunker\n",
       "from embedding.llm_adapter import LLMAdapter\n",
       "from github_interface import load_github_codebase\n",
       "from embedding.embedding import CodeEmbedding  # Import the CodeEmbedding class\n",
       "from dotenv import load_dotenv\n",
       "\n",
       "from embedding.context_wrapper import Summarizer\n",
       "from database.vector_store import VectorStore, VectorNode\n",
       "from database.snippet_database import SnippetDatabase, Snippet\n",
       "load_dotenv()\n",
       "\n",
       "class CodebaseLoader:\n",
       "    # Code replaced for brevity. See node_id ffad57b5-710b-489d-827b-ba0e3585377a\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "def main():\n",
       "    # Code replaced for brevity. See node_id 9751f071-8a46-4560-9900-c34fb8fd7234\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_python(split_nodes[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "class CodebaseLoader:\n",
       "    def __init__(self, local_dir=None, github_repo=None):\n",
       "        # Code replaced for brevity. See node_id eb9526c8-f26a-45e2-b4c6-d6ba98594788\n",
       "\n",
       "    def load_codebase(self) -> list[Snippet]:\n",
       "        # Code replaced for brevity. See node_id 7be30beb-bf49-4f6f-809e-dbd69310830c\n",
       "\n",
       "    def __test(self, txt):\n",
       "        return txt\n",
       "    \n",
       "    def extract_dir_structure(self, snippets: list[Snippet]):\n",
       "        # Code replaced for brevity. See node_id 365a6daa-c6bc-4a6b-b9da-b916fb564398\n",
       "\n",
       "    def __load_local_codebase(self, directory) -> list[Snippet]:\n",
       "        # Code replaced for brevity. See node_id 571c79ff-06ff-42a3-b87c-cd3f47fffcb4\n",
       "\n",
       "    @staticmethod\n",
       "    def is_valid_file(filepath):\n",
       "        # Code replaced for brevity. See node_id 28e2997e-6b7c-4293-b866-35ec06e4de9a```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def load_codebase(self) -> list[Snippet]:\n",
       "        if self.db.repo_exists(self.repo_id):\n",
       "            print (\"CodebaseLoader :  repo exists in relational DB\")\n",
       "            return self.db.load_snippets(self.repo_id)\n",
       "        \n",
       "        if self.github_repo:\n",
       "            self.snippets = load_github_codebase(self.github_repo)\n",
       "        elif self.local_dir:\n",
       "            self.snippets = self.__load_local_codebase(self.local_dir)\n",
       "        self.db.save_repo_dir_structure(self.repo_id, self.extract_dir_structure(self.snippets))\n",
       "        return self.__test(self.snippets)```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_python(split_nodes[1].text)\n",
    "print ('------')\n",
    "print_python(split_nodes[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Hierarchy:\n",
      "- \n",
      "  - Users\n",
      "    - shivanshj\n",
      "      - repo-personal-projects\n",
      "        - codebase2vec\n",
      "          - app\n",
      "            - CodebaseLoader\n",
      "              - __init__\n",
      "              - load_codebase\n",
      "              - extract_dir_structure\n",
      "              - __load_local_codebase\n",
      "              - is_valid_file\n",
      "            - main\n",
      "              - make_embeddings\n",
      "              - top_matches_from_vector_store\n",
      "              - generate_code\n",
      "          - code_chunker\n",
      "            - chunk_code\n",
      "            - Chunk\n",
      "              - __post_init__\n",
      "              - extract\n",
      "              - extract_lines\n",
      "              - __add__\n",
      "              - __len__\n",
      "            - _get_line_number_from_char_index\n",
      "            - TextChunker\n",
      "              - split_text\n",
      "              - _chunk_node\n",
      "              - __coalesce_chunks\n",
      "            - BlockAwareCodeSplitter\n",
      "              - __init__\n",
      "              - split_text\n",
      "              - _extract_blocks\n",
      "              - _get_block_type\n",
      "              - _get_block_name\n",
      "              - _overlaps\n",
      "            - TestBlockAwareCodeSplitter\n",
      "              - test_split_text\n",
      "              - test_get_block_type\n",
      "              - test_get_block_name\n",
      "              - test_overlaps\n",
      "          - code_graph\n",
      "            - Relation\n",
      "            - Node\n",
      "          - github_interface\n",
      "            - fetch_github_repo_contents\n",
      "            - load_github_codebase\n",
      "          - openapi_understand\n",
      "            - OpenAPIEmbedding\n",
      "            - OpenAPISpecHandler\n",
      "              - __init__\n",
      "              - _load_openapi_spec\n",
      "              - generate_embeddings\n",
      "              - _create_endpoint_embeddings\n",
      "              - find_endpoint_with_query\n",
      "              - _get_endpoint_description\n",
      "            - is_openapi_spec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hierarchy, code_structure = CodeHierarchyNodeParser.get_code_hierarchy_from_nodes(split_nodes)\n",
    "print(\"Code Hierarchy:\")\n",
    "print(code_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Query engine\n",
    "Can load by class / function name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "class CodebaseLoader:\n",
       "    def __init__(self, local_dir=None, github_repo=None):\n",
       "        # Code replaced for brevity. See node_id b35ea862-ac60-4a63-b441-3249dda0ed68\n",
       "\n",
       "    def load_codebase(self) -> list[Snippet]:\n",
       "        # Code replaced for brevity. See node_id 2211354a-788c-4191-9168-a18c3842bd4b\n",
       "\n",
       "    def __test(self, txt):\n",
       "        return txt\n",
       "    \n",
       "    def extract_dir_structure(self, snippets: list[Snippet]):\n",
       "        # Code replaced for brevity. See node_id 7e84f9b2-1b22-41e3-98b1-70534a4876cd\n",
       "\n",
       "    def __load_local_codebase(self, directory) -> list[Snippet]:\n",
       "        # Code replaced for brevity. See node_id 28adfb79-d64c-46f5-b1aa-5c26c761b292\n",
       "\n",
       "    @staticmethod\n",
       "    def is_valid_file(filepath):\n",
       "        # Code replaced for brevity. See node_id fc245a91-a80d-41d0-8b9c-8ba0bcfe41cf```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.packs.code_hierarchy import CodeHierarchyKeywordQueryEngine\n",
    "\n",
    "query_engine = CodeHierarchyKeywordQueryEngine(\n",
    "    nodes=split_nodes,\n",
    ")\n",
    "print_python(query_engine.query(\"CodebaseLoader\").response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Description: Search the tool by any element in this list to get more information about that element.\n",
       "If you see 'Code replaced for brevity' then a uuid, you may also search the tool with that uuid to see the full code.\n",
       "You may need to use the tool multiple times to fully answer the user message.\n",
       "The list is:\n",
       "- \n",
       "  - Users\n",
       "    - shivanshj\n",
       "      - repo-personal-projects\n",
       "        - codebase2vec\n",
       "          - app\n",
       "            - CodebaseLoader\n",
       "              - __init__\n",
       "              - load_codebase\n",
       "              - extract_dir_structure\n",
       "              - __load_local_codebase\n",
       "              - is_valid_file\n",
       "            - main\n",
       "              - make_embeddings\n",
       "              - top_matches_from_vector_store\n",
       "              - generate_code\n",
       "          - code_chunker\n",
       "            - chunk_code\n",
       "            - Chunk\n",
       "              - __post_init__\n",
       "              - extract\n",
       "              - extract_lines\n",
       "              - __add__\n",
       "              - __len__\n",
       "            - _get_line_number_from_char_index\n",
       "            - TextChunker\n",
       "              - split_text\n",
       "              - _chunk_node\n",
       "              - __coalesce_chunks\n",
       "            - BlockAwareCodeSplitter\n",
       "              - __init__\n",
       "              - split_text\n",
       "              - _extract_blocks\n",
       "              - _get_block_type\n",
       "              - _get_block_name\n",
       "              - _overlaps\n",
       "            - TestBlockAwareCodeSplitter\n",
       "              - test_split_text\n",
       "              - test_get_block_type\n",
       "              - test_get_block_name\n",
       "              - test_overlaps\n",
       "          - code_graph\n",
       "            - Relation\n",
       "            - Node\n",
       "          - database\n",
       "            - snippet_database\n",
       "              - SnippetDatabase\n",
       "                - __init__\n",
       "                - load_snippets\n",
       "                - get_repo_dir_structure\n",
       "                - make_repo_id\n",
       "                - save_snippet\n",
       "                - save_repo_dir_structure\n",
       "                - repo_exists\n",
       "              - TestSnippetDatabase\n",
       "                - setUp\n",
       "                - test_make_repo_id_github_url\n",
       "                - test_make_repo_id_local_path\n",
       "                - test_make_repo_id_unrecognized\n",
       "            - vector_store\n",
       "              - VectorNode\n",
       "                - __init__\n",
       "              - VectorStore\n",
       "                - __init__\n",
       "                - _connect_with_retry\n",
       "                - does_embedding_exist\n",
       "                - add_vectors\n",
       "                - search\n",
       "                - get_vectors_by_id\n",
       "                - delete_nodes\n",
       "                - _get_collection\n",
       "                - _create_collection\n",
       "                - delete_all_nodes\n",
       "              - TestVectorStore\n",
       "                - setUp\n",
       "                - test_vector_connection\n",
       "                - test_vector_search_by_id\n",
       "                - tearDown\n",
       "          - embedding\n",
       "            - context_wrapper\n",
       "              - Summarizer\n",
       "                - context_of_snippet\n",
       "                - generate_abstract_with_api\n",
       "            - embedding\n",
       "              - CodeEmbedding\n",
       "                - __init__\n",
       "                - generate_embeddings\n",
       "                - find_k_nearest_neighbors\n",
       "                - __cosine_similarity\n",
       "              - EmbeddingStrategy\n",
       "                - generate_embeddings\n",
       "              - SentenceTransformerStrategy\n",
       "                - generate_embeddings\n",
       "              - LiteLLMStrategy\n",
       "                - generate_embeddings\n",
       "              - TokenizerStrategy\n",
       "                - __init__\n",
       "                - generate_embeddings\n",
       "                - average_pool\n",
       "            - llm_adapter\n",
       "              - LLMAdapter\n",
       "                - __init__\n",
       "                - chat_completion\n",
       "          - github_interface\n",
       "            - fetch_github_repo_contents\n",
       "            - load_github_codebase\n",
       "          - openapi_understand\n",
       "            - OpenAPIEmbedding\n",
       "            - OpenAPISpecHandler\n",
       "              - __init__\n",
       "              - _load_openapi_spec\n",
       "              - generate_embeddings\n",
       "              - _create_endpoint_embeddings\n",
       "              - find_endpoint_with_query\n",
       "              - _get_endpoint_description\n",
       "            - is_openapi_spec\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"code_lookup\",\n",
    "    description=\"Useful for looking up information about the code hierarchy codebase.\",\n",
    ")\n",
    "\n",
    "display(Markdown(\"Description: \" + query_engine.get_tool_instructions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.packs.code_hierarchy import CodeHierarchyAgentPack\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4\", temperature=0.1)\n",
    "\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [tool], llm=llm, system_prompt=query_engine.get_tool_instructions(), verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: How does the codebaseloader function work? Provide specific implementation details.\n",
      "=== Calling Function ===\n",
      "Calling function: code_lookup with args: {\n",
      "  \"input\": \"CodebaseLoader\"\n",
      "}\n",
      "Got output: class CodebaseLoader:\n",
      "    def __init__(self, local_dir=None, github_repo=None):\n",
      "        # Code replaced for brevity. See node_id b35ea862-ac60-4a63-b441-3249dda0ed68\n",
      "\n",
      "    def load_codebase(self) -> list[Snippet]:\n",
      "        # Code replaced for brevity. See node_id 2211354a-788c-4191-9168-a18c3842bd4b\n",
      "\n",
      "    def __test(self, txt):\n",
      "        return txt\n",
      "    \n",
      "    def extract_dir_structure(self, snippets: list[Snippet]):\n",
      "        # Code replaced for brevity. See node_id 7e84f9b2-1b22-41e3-98b1-70534a4876cd\n",
      "\n",
      "    def __load_local_codebase(self, directory) -> list[Snippet]:\n",
      "        # Code replaced for brevity. See node_id 28adfb79-d64c-46f5-b1aa-5c26c761b292\n",
      "\n",
      "    @staticmethod\n",
      "    def is_valid_file(filepath):\n",
      "        # Code replaced for brevity. See node_id fc245a91-a80d-41d0-8b9c-8ba0bcfe41cf\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: code_lookup with args: {\n",
      "  \"input\": \"b35ea862-ac60-4a63-b441-3249dda0ed68\"\n",
      "}\n",
      "Got output: def __init__(self, local_dir=None, github_repo=None):\n",
      "        self.local_dir = local_dir\n",
      "        self.github_repo = github_repo\n",
      "        self.db = SnippetDatabase()\n",
      "        self.repo_id = self.db.make_repo_id(self.github_repo)\n",
      "        self.snippets = []\n",
      "        self.directory_structure = None\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: code_lookup with args: {\n",
      "  \"input\": \"2211354a-788c-4191-9168-a18c3842bd4b\"\n",
      "}\n",
      "Got output: def load_codebase(self) -> list[Snippet]:\n",
      "        if self.db.repo_exists(self.repo_id):\n",
      "            print (\"CodebaseLoader :  repo exists in relational DB\")\n",
      "            return self.db.load_snippets(self.repo_id)\n",
      "        \n",
      "        if self.github_repo:\n",
      "            self.snippets = load_github_codebase(self.github_repo)\n",
      "        elif self.local_dir:\n",
      "            self.snippets = self.__load_local_codebase(self.local_dir)\n",
      "        self.db.save_repo_dir_structure(self.repo_id, self.extract_dir_structure(self.snippets))\n",
      "        return self.__test(self.snippets)\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: code_lookup with args: {\n",
      "  \"input\": \"7e84f9b2-1b22-41e3-98b1-70534a4876cd\"\n",
      "}\n",
      "Got output: def extract_dir_structure(self, snippets: list[Snippet]):\n",
      "        if dir := self.db.get_repo_dir_structure(self.repo_id):\n",
      "            print (\"CodebaseLoader :  dir exists in relational DB\")\n",
      "            return dir\n",
      "        dir_structure = '\\n'\n",
      "        for snippet in snippets:\n",
      "            dir_structure += snippet.file_path\n",
      "            dir_structure += '\\n'\n",
      "        return dir_structure\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: code_lookup with args: {\n",
      "  \"input\": \"28adfb79-d64c-46f5-b1aa-5c26c761b292\"\n",
      "}\n",
      "Got output: def __load_local_codebase(self, directory) -> list[Snippet]:\n",
      "        snippets = []\n",
      "        for filename in os.listdir(directory):\n",
      "            if filename.startswith('.'):\n",
      "                continue\n",
      "            filepath = os.path.join(directory, filename)\n",
      "            if os.path.isdir(filepath):\n",
      "                snippets.extend(self.__load_local_codebase(filepath))\n",
      "            else:\n",
      "                if self.is_valid_file(filepath):\n",
      "                    with open(filepath, 'r') as file:\n",
      "                        content = file.read().strip()\n",
      "                        if content:\n",
      "                            newSnippet = Snippet(content=content, file_path=filepath)\n",
      "                            snippets.append(newSnippet)\n",
      "                            self.db.save_snippet(self.repo_id, newSnippet)\n",
      "        return snippets\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: code_lookup with args: {\n",
      "  \"input\": \"fc245a91-a80d-41d0-8b9c-8ba0bcfe41cf\"\n",
      "}\n",
      "Got output: def is_valid_file(filepath):\n",
      "        IGNORED_FILES = [\"package-lock.json\", \"yarn.lock\", \"poetry.lock\"]\n",
      "        ALLOWED_EXTENSIONS = [\".py\", \".tsx\"]\n",
      "        return (not any(ignored in filepath for ignored in IGNORED_FILES) and\n",
      "                any(filepath.endswith(ext) for ext in ALLOWED_EXTENSIONS))\n",
      "========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"How does the codebaseloader function work? Provide specific implementation details.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `CodebaseLoader` class is responsible for loading a codebase from a given repository. Here's how it works:\n",
      "\n",
      "1. **Initialization (`__init__`)**: The constructor takes two optional arguments: `local_dir` and `github_repo`. These represent the local directory and the GitHub repository from which the codebase is to be loaded. The specific implementation details of the constructor have been replaced for brevity.\n",
      "\n",
      "2. **Loading the Codebase (`load_codebase`)**: This method loads the codebase from the repository. It first checks if the repository exists in the database. If it does, it loads the snippets from the database. If it doesn't, it checks if the repository is a GitHub repository or a local directory and loads the codebase accordingly. It then saves the directory structure of the repository to the database and returns the loaded snippets.\n",
      "\n",
      "3. **Testing (`__test`)**: This method is used to test the loaded snippets. The specific implementation details have been replaced for brevity.\n",
      "\n",
      "4. **Extracting Directory Structure (`extract_dir_structure`)**: This method extracts the directory structure of the repository from the loaded snippets. The specific implementation details have been replaced for brevity.\n",
      "\n",
      "5. **Loading Local Codebase (`__load_local_codebase`)**: This method loads the codebase from a local directory. The specific implementation details have been replaced for brevity.\n",
      "\n",
      "6. **Checking Validity of File (`is_valid_file`)**: This static method checks if a given file path is valid. The specific implementation details have been replaced for brevity.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    color: red;\n",
    "    border: 1px solid #ddd;\n",
    "    border-radius: 4px;\n",
    "    padding: 15px;\n",
    "    margin: 10px 0;\">\n",
    "\n",
    "### Conclusion:\n",
    "</div>\n",
    "\n",
    "1. Llama-index doesn't work on empty files\n",
    "\n",
    "2. It can't provide abstracts of a function within a function.\n",
    "    a. Example:\n",
    "    ```python\n",
    "    def func_A():\n",
    "        return func_B()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
